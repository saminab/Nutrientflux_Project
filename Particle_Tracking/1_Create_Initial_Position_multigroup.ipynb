{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the initial position file\n",
    " #### The following code read the pourpoints extracted from Arc gis in a way that.so in each watershed we have one point. \n",
    " Then using Pylag create initial position file it will create a release zone for each pour point/groupid, in the end each group_id will contain sets of release zone \n",
    " 6/3/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1-1: import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "import matplotlib.pyplot as plt\n",
    "from pylag.processing.coordinate import get_epsg_code, utm_from_lonlat, lonlat_from_utm\n",
    "from pylag.processing.coordinate import utm_from_lonlat, lonlat_from_utm\n",
    "from pylag.processing.release_zone import create_release_zones_along_cord\n",
    "from pylag.processing.plot import create_figure, colourmap\n",
    "from pylag.processing.plot import FVCOMPlotter\n",
    "from pylag.processing.release_zone import create_release_zone\n",
    "from pylag.processing.input import create_initial_positions_file_multi_group\n",
    "from pylag.processing.input import create_initial_positions_file_single_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part1-2: read the initial position file and rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the input file\n",
    "data_dir ='/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/input/initial_position'\n",
    "init_file = os.path.join(data_dir, 'WatershedPourPoints_LH_copy_multigroup.dat')\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# Create run directory\n",
    "simulation_dir = '/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron'.format(cwd)\n",
    "try:\n",
    "    os.makedirs(simulation_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "df = pd.read_csv(init_file, skiprows=0,sep=' ',header=None)\n",
    "\n",
    "df.rename(columns={0:\"group_id\", 1:\"lon\",2:\"lat\",3:\"depth\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-84.665841</td>\n",
       "      <td>45.745061</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-84.828741</td>\n",
       "      <td>45.749943</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-84.486036</td>\n",
       "      <td>45.663177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-84.234358</td>\n",
       "      <td>45.637350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-84.464815</td>\n",
       "      <td>45.655921</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>128</td>\n",
       "      <td>-83.425968</td>\n",
       "      <td>45.061532</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>129</td>\n",
       "      <td>-84.078374</td>\n",
       "      <td>45.487312</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>130</td>\n",
       "      <td>-83.396562</td>\n",
       "      <td>43.818410</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>135</td>\n",
       "      <td>-82.406943</td>\n",
       "      <td>43.030980</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>147</td>\n",
       "      <td>-83.465788</td>\n",
       "      <td>43.738634</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     group_id        lon        lat  depth\n",
       "0           0 -84.665841  45.745061    0.0\n",
       "1           1 -84.828741  45.749943    0.0\n",
       "2           2 -84.486036  45.663177    0.0\n",
       "3           3 -84.234358  45.637350    0.0\n",
       "4           4 -84.464815  45.655921    0.0\n",
       "..        ...        ...        ...    ...\n",
       "127       128 -83.425968  45.061532    0.0\n",
       "128       129 -84.078374  45.487312    0.0\n",
       "129       130 -83.396562  43.818410    0.0\n",
       "130       135 -82.406943  43.030980    0.0\n",
       "131       147 -83.465788  43.738634    0.0\n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lon'] = df['lon']-360\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insure that all the points are placed inside lake boundary\n",
    "update the function to check if the point is within the lake's boundary polygon rather than just near it. You can use the contains method of the polygon to determine if the point is inside the lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I want to keep the FVCOME nodes that are only have overlap with Lake_huron boundary and Erase the nodes that are noT inside lake huron and save a new FVCOME node with the name of Lake_HUron_node.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Lake Huron shapefile\n",
    "lake_huron_boundary  = gpd.read_file('/mnt/d/Users/abolmaal/Arcgis/NASAOceanProject/GIS_layer/Basins/hydro_p_LakeHuron/hydro_p_LakeHuron.shp')\n",
    "# load FVCOM nodes that extracted from the FVCOM model\n",
    "fvcom_nodes = gpd.read_file('/mnt/d/Users/abolmaal/Arcgis/NASAOceanProject/GIS_layer/Basins/FVCOME/fvcomenodes.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both shapefiles are in the same CRS\n",
    "if fvcom_nodes.crs != lake_huron_boundary.crs:\n",
    "    lake_huron_boundary = lake_huron_boundary.to_crs(fvcom_nodes.crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial join to keep only the nodes that intersect with Lake Huron boundary\n",
    "lake_huron_nodes = lake_huron_boundary.sjoin(fvcom_nodes, how='inner', predicate='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any unnecessary columns added by the spatial join\n",
    "lake_huron_nodes = lake_huron_nodes.drop(columns=['index_right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the node that are not in the CANADA\n",
    "lake_huron_nodes = lake_huron_nodes[lake_huron_nodes['COUNTRY'] != 'CAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the nodes\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "lake_huron_boundary.plot(ax=ax, color='blue', edgecolor='black',label = 'Lake Huron Boundary')\n",
    "lake_huron_nodes.plot(ax=ax, color='red', edgecolor='red', label = 'FVCOM Nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lake_huron_nodes to a shapefile with output name of lake_huron_nodes.shp in the same directory as the input shapefile\n",
    "lake_huron_nodes.to_file('/mnt/d/Users/abolmaal/Arcgis/NASAOceanProject/GIS_layer/Basins/lake_huron_fvcomenodes.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read lake_huron_nodes shapefile\n",
    "lake_huron_nodes_fvcome = gpd.read_file('/mnt/d/Users/abolmaal/Arcgis/NASAOceanProject/GIS_layer/Basins/lake_huron_fvcomenodes.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have this geodatabase lake_huron_nodes_fvcome that has geometry and CRS now I want to update the following code in a way add a condition to CReate_release zone for that point that have overlap or intersect with lake_huron_nodes_fvcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_225774/2103050781.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  lake_huron_nodes_fvcome['centroid'] = lake_huron_nodes_fvcome.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       HYDRO_P_  UIDENT  TYPE COUNTRY      NAMEEN                NAMESP  \\\n",
      "20059      3261  552202    16     USA  Lake Huron  Lake Huron/Lac Huron   \n",
      "20060      3261  552202    16     USA  Lake Huron  Lake Huron/Lac Huron   \n",
      "20061      3261  552202    16     USA  Lake Huron  Lake Huron/Lac Huron   \n",
      "20062      3261  552202    16     USA  Lake Huron  Lake Huron/Lac Huron   \n",
      "20063      3261  552202    16     USA  Lake Huron  Lake Huron/Lac Huron   \n",
      "\n",
      "          NAMEFR  InstanceID  OID_1    x    y  \\\n",
      "20059  Lac Huron       51741  51740  0.0  0.0   \n",
      "20060  Lac Huron       51488  51487  0.0  0.0   \n",
      "20061  Lac Huron       51613  51612  0.0  0.0   \n",
      "20062  Lac Huron       51617  51616  0.0  0.0   \n",
      "20063  Lac Huron       51618  51617  0.0  0.0   \n",
      "\n",
      "                                                geometry  \\\n",
      "20059  POLYGON ((-84.11286 46.32809, -84.10651 46.321...   \n",
      "20060  POLYGON ((-84.11286 46.32809, -84.10651 46.321...   \n",
      "20061  POLYGON ((-84.11286 46.32809, -84.10651 46.321...   \n",
      "20062  POLYGON ((-84.11286 46.32809, -84.10651 46.321...   \n",
      "20063  POLYGON ((-84.11286 46.32809, -84.10651 46.321...   \n",
      "\n",
      "                         centroid        lon        lat  \n",
      "20059  POINT (-83.13191 44.74672) -83.131908  44.746722  \n",
      "20060  POINT (-83.13191 44.74672) -83.131908  44.746722  \n",
      "20061  POINT (-83.13191 44.74672) -83.131908  44.746722  \n",
      "20062  POINT (-83.13191 44.74672) -83.131908  44.746722  \n",
      "20063  POINT (-83.13191 44.74672) -83.131908  44.746722  \n"
     ]
    }
   ],
   "source": [
    "lake_huron_nodes_fvcome['centroid'] = lake_huron_nodes_fvcome.centroid\n",
    "\n",
    "# Extract the longitude (x) and latitude (y) from the centroid\n",
    "lake_huron_nodes_fvcome['lon'] = lake_huron_nodes_fvcome['centroid'].x\n",
    "lake_huron_nodes_fvcome['lat'] = lake_huron_nodes_fvcome['centroid'].y\n",
    "\n",
    "# Optionally, you can drop the 'centroid' column if it's no longer needed\n",
    "# lake_huron_nodes_fvcome = lake_huron_nodes_fvcome.drop(columns=['centroid'])\n",
    "\n",
    "# Display the updated GeoDataFrame\n",
    "print(lake_huron_nodes_fvcome.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lat/lon points in df to a GeoDataFrame\n",
    "# Assuming you have a DataFrame 'df' with 'lon' and 'lat' columns\n",
    "df['geometry'] = df.apply(lambda row: Point(row['lon'], row['lat']), axis=1)\n",
    "points_gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Ensure lake_huron_nodes_fvcome has the same CRS\n",
    "if lake_huron_nodes_fvcome.crs != 'EPSG:4326':\n",
    "    lake_huron_nodes_fvcome = lake_huron_nodes_fvcome.to_crs('EPSG:4326')\n",
    "\n",
    "# Create a spatial index for lake_huron_nodes_fvcome to speed up intersection checks\n",
    "lake_huron_nodes_fvcome_sindex = lake_huron_nodes_fvcome.sindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pylag/lib/python3.11/site-packages/pandas/core/indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pylag/lib/python3.11/site-packages/pandas/core/generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4145\u001b[0m \u001b[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   4146\u001b[0m \u001b[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4151\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4153\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4154\u001b[0m \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pylag/lib/python3.11/site-packages/pandas/core/generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[1;32m   4131\u001b[0m     )\n\u001b[0;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4140\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pylag/lib/python3.11/site-packages/pandas/core/internals/managers.py:891\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    890\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[0;32m--> 891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[0;32m~/anaconda3/envs/pylag/lib/python3.11/site-packages/pandas/core/indexers/utils.py:282\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(points_gdf), chunk_size):\n\u001b[1;32m     23\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m points_gdf\u001b[38;5;241m.\u001b[39miloc[i:i \u001b[38;5;241m+\u001b[39m chunk_size]\n\u001b[0;32m---> 24\u001b[0m     intersecting_points \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_intersecting_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlake_huron_nodes_fvcome\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Loop through the filtered intersecting points\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m intersecting_points\u001b[38;5;241m.\u001b[39miterrows():\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mfilter_intersecting_points\u001b[0;34m(points_gdf, lake_huron_nodes_fvcome)\u001b[0m\n\u001b[1;32m     11\u001b[0m possible_matches_index \u001b[38;5;241m=\u001b[39m points_gdf\u001b[38;5;241m.\u001b[39msindex\u001b[38;5;241m.\u001b[39mquery(lake_huron_nodes_fvcome\u001b[38;5;241m.\u001b[39mgeometry, predicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintersects\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Perform a spatial join to retain only intersecting points\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m intersecting_points \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39msjoin(\u001b[43mpoints_gdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpossible_matches_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, lake_huron_nodes_fvcome, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m, predicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintersects\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m intersecting_points\n",
      "File \u001b[0;32m~/anaconda3/envs/pylag/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pylag/lib/python3.11/site-packages/pandas/core/indexing.py:1743\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1747\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/pylag/lib/python3.11/site-packages/pandas/core/indexing.py:1717\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[0;32m-> 1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Release zone parameters\n",
    "radius = 0.01\n",
    "n_particles_target = 100\n",
    "depth_below_surface = 0.0\n",
    "release_zones = []\n",
    "\n",
    "\n",
    "# Function to filter points that intersect with lake_huron_nodes_fvcome\n",
    "def filter_intersecting_points(points_gdf, lake_huron_nodes_fvcome):\n",
    "    # Perform the spatial query using the actual geometries (not bounds)\n",
    "    possible_matches_index = points_gdf.sindex.query(lake_huron_nodes_fvcome.geometry, predicate=\"intersects\")\n",
    "    \n",
    "    # Perform a spatial join to retain only intersecting points\n",
    "    intersecting_points = gpd.sjoin(points_gdf.iloc[possible_matches_index[0]], lake_huron_nodes_fvcome, how='inner', predicate='intersects')\n",
    "    \n",
    "    return intersecting_points\n",
    "\n",
    "# Process points in chunks if needed\n",
    "chunk_size = 1000  # Adjust chunk size based on available memory\n",
    "release_zones = []\n",
    "\n",
    "for i in range(0, len(points_gdf), chunk_size):\n",
    "    chunk = points_gdf.iloc[i:i + chunk_size]\n",
    "    intersecting_points = filter_intersecting_points(chunk, lake_huron_nodes_fvcome)\n",
    "    \n",
    "    # Loop through the filtered intersecting points\n",
    "    for index, row in intersecting_points.iterrows():\n",
    "        group_id = row['group_id']\n",
    "        lon = row['geometry'].x\n",
    "        lat = row['geometry'].y\n",
    "        \n",
    "        # Create the release zone only for points that intersect the boundary\n",
    "        surface_release_zone = create_release_zone(group_id=group_id,\n",
    "                                                   radius=radius,\n",
    "                                                   centre=[lon, lat],\n",
    "                                                   n_particles=n_particles_target,\n",
    "                                                   depth=depth_below_surface,\n",
    "                                                   random=False)\n",
    "        \n",
    "        # Accumulate the release zones\n",
    "        release_zones.append(surface_release_zone)\n",
    "\n",
    "# Create input sub-directory\n",
    "input_dir = os.path.join(simulation_dir, 'input/initial_position')\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "# Output filename\n",
    "file_name = os.path.join(input_dir, 'WatershedPourPoints_LH_multigroup_4.dat')\n",
    "\n",
    "# Write data to file\n",
    "create_initial_positions_file_multi_group(file_name, release_zones)\n",
    "\n",
    "print(f\"Saved release zones to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2: Create a release zone and save it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release zone  \n",
    "radius = 0.01\n",
    "# target number of particles\n",
    "n_particles_target = 100\n",
    "\n",
    "# Release depths\n",
    "depth_below_surface = 0.0\n",
    "# list to accumulate release zones\n",
    "release_zones = []\n",
    "\n",
    "# Loop through the rows of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    group_id = row['group_id']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    \n",
    "    # Create the release zone\n",
    "    surface_release_zone = create_release_zone(group_id = group_id,\n",
    "                                           radius = radius,\n",
    "                                           centre = [lon, lat],\n",
    "                                           n_particles = n_particles_target,\n",
    "                                           depth = depth_below_surface,\n",
    "                                           random = False)\n",
    "    \n",
    "    # accumulate the release zones\n",
    "    release_zones.append(surface_release_zone)\n",
    "    \n",
    "    \n",
    " # Create input sub-directory\n",
    "input_dir = os.path.join(simulation_dir, 'input/initial_position')\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "    # Output filename\n",
    "file_name = os.path.join(input_dir, 'WatershedPourPoints_LH_multigroup_3.dat')\n",
    "# Write data to file\n",
    "create_initial_positions_file_multi_group(file_name, release_zones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve an error related to PyLag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Lat and lon number has more than 10 digits and when I run it through Pylag it gives me this error:ValueError: invalid literal for int() with base 10: '0.0' so to avoid this I want to read Lat and Lon and round them by 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file_name using pandas\n",
    "from decimal import Decimal\n",
    "df = pd.read_csv(file_name, skiprows=1,sep=' ',header=None)\n",
    "df.rename(columns={0:\"group_id\", 1:\"lon\",2:\"lat\",3:\"depth\"}, inplace=True)\n",
    "# conver lon to lon + 360\n",
    "df['lon'] = df['lon'] + 360\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if df[lon] and df[lat] are greater than 10 digits round them to 10 digits\n",
    "df['lon'] = df['lon'].round(10)\n",
    "df['lat'] = df['lat'].round(10)\n",
    "df['group_id'] = df['group_id'].astype(int)\n",
    "# remove the header and write the data to the file name called WatershedPourPoints_LH_multigroup_rounded_inisidelake.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pandas dataframe to the file name called WatershedPourPoints_LH_multigroup_rounded.dat\n",
    "file_name_new = os.path.join(input_dir, 'WatershedPourPoints_LH_multigroup_rounded_insidelake.dat')\n",
    "df.to_csv(file_name_new, sep=' ', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
