{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following Code reads FVCOM particle tracking outputs and get the group_number\n",
    "- Count the number if group_id in each group\n",
    "- Add a new column to the data called group_number and add the number of particle in each group_id \n",
    "- The order is the first number is group_is,second numbers are particle number \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory of the FVCOM model outputs\n",
    "FVCOM_dir = '/home/abolmaal/modelling/FVCOM/Huron/output'\n",
    "# Set the directory of the FVCOM model outputs\n",
    "files = glob.glob(os.path.join(FVCOM_dir, 'FVCOM_Huron_2323_*.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to sort the files based on the time\n",
    "def sort_key(file):\n",
    "    filename = os.path.basename(file)\n",
    "    try:\n",
    "        # Extract the number after the double underscores and before the `.nc` extension\n",
    "        number = int(filename.split('_')[-1].split('.')[0])\n",
    "        return number\n",
    "    except (IndexError, ValueError):\n",
    "        # Handle filenames that do not match the pattern by returning a high number to place them last\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Function: group_id + particle_index + yymmddHHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Output\n",
    "group_id = 1, group_index = 5, release_time = 2301 ‚Üí particle_id = 001052301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def write_particle_id(files, data_dir):\n",
    "    \"\"\"\n",
    "    Adds a particle_id = group_id(3-digit) + group_index(2-digit) + release_time(4-digit) to each NetCDF file.\n",
    "    Also adds a group_number field if needed (optional).\n",
    "    \"\"\"\n",
    "\n",
    "    release_times = [\n",
    "        \"2301\", \"2302\", \"2303\", \"2304\",\n",
    "        \"2305\", \"2306\", \"2307\", \"2308\",\n",
    "        \"2309\", \"2310\", \"2311\", \"2312\"\n",
    "    ]\n",
    "    # release_times = [\n",
    "    #     \"2401\", \"2402\", \"2403\", \"2404\",\n",
    "    #     \"2405\", \"2406\", \"2407\", \"2408\",\n",
    "    #     \"2409\", \"2410\", \"2411\", \"2412\"\n",
    "    # ]\n",
    "\n",
    "    for idx, file in enumerate(files):\n",
    "        print(f\"üîÑ Processing: {os.path.basename(file)}\")\n",
    "\n",
    "        ds = xr.open_dataset(file)\n",
    "        num_particles = ds.sizes['particles']\n",
    "\n",
    "        # Get group_id values\n",
    "        group_ids = ds['group_id'].values\n",
    "        release_time = release_times[idx]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({'group_id': group_ids})\n",
    "        df['group_index'] = df.groupby('group_id').cumcount()\n",
    "\n",
    "        # Create particle_id with padding\n",
    "        df['particle_id'] = df.apply(\n",
    "            lambda row: f\"{int(row['group_id']):03}{int(row['group_index']):02}{release_time}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Safety check\n",
    "        if len(df) != num_particles:\n",
    "            raise ValueError(f\"‚ùå Length mismatch in {file}: {len(df)} particle_ids vs {num_particles} particles\")\n",
    "\n",
    "        # Assign to dataset\n",
    "        ds['particle_id'] = (('particles'), df['particle_id'].values.astype('U15'))\n",
    "\n",
    "        # Save updated file\n",
    "        output_path = os.path.join(data_dir, f\"updated_{os.path.basename(file)}\")\n",
    "        ds.to_netcdf(output_path)\n",
    "        print(f\"‚úÖ Saved: {output_path}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/abolmaal/modelling/FVCOM/Huron/output/FVCOM_Huron_2323_JanFeb_1.nc']\n"
     ]
    }
   ],
   "source": [
    "# Load the FVCOM output files and sort them based on the time\n",
    "files = glob.glob(FVCOM_dir + \"/FVCOM_Huron_2323_*.nc\")\n",
    "files.sort(key=sort_key)\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing: FVCOM_Huron_2323_JanFeb_1.nc\n",
      "‚úÖ Saved: /home/abolmaal/modelling/FVCOM/Huron/output/updated_FVCOM_Huron_2323_JanFeb_1.nc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to update the NetCDF files with the group_number column\n",
    "write_particle_id(files, FVCOM_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé First 10 particle_id values:\n",
      "['000002301' '000012301' '000022301' '000032301' '000042301' '000052301'\n",
      " '000062301' '000072301' '000082301' '000092301' '000102301' '000112301'\n",
      " '000122301' '000132301' '000142301' '000152301' '000162301' '000172301'\n",
      " '000182301' '000192301' '000202301' '000212301' '000222301' '000232301'\n",
      " '000242301' '000252301' '000262301' '000272301' '000282301' '000292301'\n",
      " '000302301' '000312301' '000322301' '000332301' '000342301' '000352301'\n",
      " '000362301' '000372301' '000382301' '000392301' '000402301' '000412301'\n",
      " '000422301' '000432301' '000442301' '000452301' '000462301' '000472301'\n",
      " '000482301' '000492301' '000502301' '000512301' '000522301' '000532301'\n",
      " '000542301' '000552301' '000562301' '000572301' '000582301' '000592301'\n",
      " '000602301' '000612301' '000622301' '000632301' '000642301' '000652301'\n",
      " '000662301' '000672301' '000682301' '000692301' '000702301' '000712301'\n",
      " '000722301' '000732301' '000742301' '003002301' '003012301' '003022301'\n",
      " '003032301' '003042301' '003052301' '003062301' '003072301' '003082301'\n",
      " '003092301' '003102301' '003112301' '003122301' '003132301' '003142301'\n",
      " '003152301' '003162301' '003172301' '003182301' '003192301' '003202301'\n",
      " '003212301' '003222301' '003232301' '003242301' '003252301' '003262301'\n",
      " '003272301' '003282301' '003292301' '003302301' '003312301' '003322301'\n",
      " '003332301' '003342301' '003352301' '003362301' '003372301' '003382301'\n",
      " '003392301' '003402301' '003412301' '003422301' '003432301' '003442301'\n",
      " '003452301' '003462301' '003472301' '003482301' '003492301' '003502301'\n",
      " '003512301' '003522301' '003532301' '003542301' '003552301' '003562301'\n",
      " '003572301' '003582301' '003592301' '003602301' '003612301' '003622301'\n",
      " '003632301' '003642301' '003652301' '003662301' '003672301' '003682301'\n",
      " '003692301' '003702301' '003712301' '003722301' '003732301' '003742301'\n",
      " '004002301' '004012301']\n"
     ]
    }
   ],
   "source": [
    "# Make sure the group_number column is added to the NetCDF files and is in right order \n",
    "# read the updated NetCDF file\n",
    "updated_files = glob.glob(FVCOM_dir + \"/updated_FVCOM_Huron_2323_*.nc\")\n",
    "updated_files.sort(key=sort_key)\n",
    "# Open the first file\n",
    "ds = xr.open_dataset(updated_files[0])\n",
    "\n",
    "# Print the first 10 particle_id values\n",
    "print(\"üîé First 10 particle_id values:\")\n",
    "print(ds['particle_id'].values[:152])\n",
    "# # read files usimg xarray\n",
    "# ds = xr.open_mfdataset(updated_files, combine='nested', concat_dim='time', parallel=True)\n",
    "\n",
    "# # read the firs file\n",
    "# #ds = xr.open_dataset(updated_files[:])\n",
    "# # print the updated group_number variable values\n",
    "# print(ds['group_number'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to read each file adn 1- # Step 1: Convert time to datetime (safe for large data)\n",
    "# Step 2: Extract day and month period\n",
    "# Step 3: Count unique days per month (memory-efficient)\n",
    "# Step_ 4: Create a DataFrame with the results\n",
    "def count_unique_days_per_month(files):\n",
    "    \"\"\"\n",
    "    Counts unique days per month across multiple NetCDF files.\n",
    "    Returns a DataFrame with the results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for file in files:\n",
    "        print(f\"üîÑ Processing: {os.path.basename(file)}\")\n",
    "        with xr.open_dataset(file) as ds:  # Use context manager\n",
    "\n",
    "            # Convert time to datetime\n",
    "            time = pd.to_datetime(ds['time'].values, unit='s', origin='unix')\n",
    "\n",
    "            # Extract day and month\n",
    "            days = time.day\n",
    "            months = time.month\n",
    "\n",
    "            # Create a DataFrame for counting unique days per month\n",
    "            df = pd.DataFrame({'day': days, 'month': months})\n",
    "            unique_days_per_month = df.groupby('month')['day'].nunique().reset_index()\n",
    "\n",
    "            # Append results\n",
    "            results.append(unique_days_per_month)\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    final_counts = final_df.groupby('month')['day'].sum().reset_index()\n",
    "    # save the final counts to a CSV file\n",
    "    output_csv = os.path.join(FVCOM_dir, 'days_per_releasetime_24.csv')\n",
    "    final_counts.to_csv(output_csv, index=False)\n",
    "    return final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing: updated_FVCOM_Huron_2424_JanFeb_1.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_FebMar_2.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_MarApr_3.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_AprMay_4.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_MayJun_5.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_JunJul_6.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_JulAug_7.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_AugSep_8.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_SepOct_9.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_OctNov_10.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_NovDec_11.nc\n"
     ]
    }
   ],
   "source": [
    "#  Call the function to count unique days per month\n",
    "unique_days_per_month = count_unique_days_per_month(updated_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the occurance of each group_id in ds using xarray\n",
    "group_id_counts = ds['group_id'].count(dim='particles').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year and month as 'YYYY-MM' from 'time'\n",
    "ds['month'] = ds['time'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = ds.to_dataframe()\n",
    "\n",
    "# Group by 'month' and 'group_id', and count the occurrences\n",
    "group_id_counts_df = df.groupby(['month', 'group_id']).size().reset_index(name='group_id_count')\n",
    "\n",
    "# Pivot the table to have months as rows and group_ids as columns\n",
    "pivot_df = group_id_counts_df.pivot(index='month', columns='group_id', values='group_id_count').fillna(0)\n",
    "\n",
    "# Print the resulting pivoted DataFrame\n",
    "print(pivot_df)\n",
    "#save this pivoted DataFrame to a csv file\n",
    "output_csv_path = os.path.join(FVCOM_dir, 'group_id_counts_by_month.csv')\n",
    "pivot_df.to_csv(output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a DataFrame for easier manipulation\n",
    "df = ds.to_dataframe().reset_index()  # Resetting the index to make 'time' a column\n",
    "\n",
    "# Check the first few rows to confirm the structure of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Filter the data for the specific day and group_id = 0\n",
    "filtered_df = df[(df['month'] == '2023-01') & (df['group_id'] == 0)]\n",
    "\n",
    "# Count the occurrences of group_id = 0 on the day 2023-01-01\n",
    "occurrences = filtered_df[filtered_df['time'] == '2023-01-01'].shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Occurrences of group_id=0 on 2023-01: {occurrences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a DataFrame for easier manipulation\n",
    "df = ds.to_dataframe().reset_index()  # Resetting the index to make 'time' a column\n",
    "\n",
    "# Check the first few rows to confirm the structure of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Filter the data for group_id = 0\n",
    "filtered_df = df[df['group_id'] == 0].copy()  # Use copy() to avoid warning\n",
    "\n",
    "# Convert the 'time' column to just the date (ignoring the time part)\n",
    "filtered_df['date'] = filtered_df['time'].dt.date  # Extract date without time\n",
    "\n",
    "# Extract month and year for filtering\n",
    "filtered_df['month'] = filtered_df['time'].dt.to_period('M')  # Use .loc to avoid warning\n",
    "\n",
    "# Filter data for the month of January 2023\n",
    "filtered_january = filtered_df[filtered_df['month'] == '2023-02']\n",
    "\n",
    "# Now count the total occurrences of group_id = 0 for the entire month of January 2023\n",
    "total_occurrences = filtered_january.shape[0]  # Count all rows\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total occurrences of group_id=0 in January 2023: {total_occurrences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NetCDF 'group_id' and 'group_number' variables to a DataFrame for processing\n",
    "netcdf_df = ds['group_id'].to_dataframe().reset_index()\n",
    "\n",
    "# Ensure that 'group_number' is extracted and correctly added\n",
    "netcdf_df['group_number'] = ds['group_number'].values\n",
    "\n",
    "# Step 1: Convert 'group_number' to integer if necessary, and format as a 5-digit string with leading zeros\n",
    "netcdf_df['group_number'] = netcdf_df['group_number'].apply(lambda x: f\"{int(x):11}\")\n",
    "\n",
    "# Step 2: Print values for debugging\n",
    "print(\"First few rows of netcdf_df:\")\n",
    "print(netcdf_df.head())\n",
    "\n",
    "# Step 3: Select relevant columns for saving to CSV\n",
    "netcdf_df = netcdf_df[['group_id', 'group_number']]\n",
    "\n",
    "# Save the result as a CSV file\n",
    "netcdf_df.to_csv(os.path.join(FVCOM_dir, 'group_id_group_number.csv'), index=False)\n",
    "\n",
    "print(\"CSV file saved with formatted group numbers\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylag_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
