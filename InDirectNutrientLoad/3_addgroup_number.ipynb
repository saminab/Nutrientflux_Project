{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following Code reads FVCOM particle tracking outputs and get the group_number\n",
    "- Count the number if group_id in each group\n",
    "- Add a new column to the data called group_number and add the number of particle in each group_id \n",
    "- The order is the first number is group_is,second numbers are particle number \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory of the FVCOM model outputs\n",
    "FVCOM_dir = '/home/abolmaal/modelling/FVCOM/Huron/output'\n",
    "# Set the directory of the FVCOM model outputs\n",
    "files = glob.glob(os.path.join(FVCOM_dir, 'FVCOM_Huron_2424_*.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to sort the files based on the time\n",
    "def sort_key(file):\n",
    "    filename = os.path.basename(file)\n",
    "    try:\n",
    "        # Extract the number after the double underscores and before the `.nc` extension\n",
    "        number = int(filename.split('_')[-1].split('.')[0])\n",
    "        return number\n",
    "    except (IndexError, ValueError):\n",
    "        # Handle filenames that do not match the pattern by returning a high number to place them last\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Function: group_id + particle_index + yymmddHHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Output\n",
    "group_id = 1, group_index = 5, release_time = 2301 ‚Üí particle_id = 001052301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def write_particle_id(files, data_dir):\n",
    "    \"\"\"\n",
    "    Adds a particle_id = group_id(3-digit) + group_index(2-digit) + release_time(4-digit) to each NetCDF file.\n",
    "    Also adds a group_number field if needed (optional).\n",
    "    \"\"\"\n",
    "\n",
    "    # release_times = [\n",
    "    #     \"2301\", \"2302\", \"2303\", \"2304\",\n",
    "    #     \"2305\", \"2306\", \"2307\", \"2308\",\n",
    "    #     \"2309\", \"2310\", \"2311\", \"2312\"\n",
    "    # ]\n",
    "    release_times = [\n",
    "        \"2401\", \"2402\", \"2403\", \"2404\",\n",
    "        \"2405\", \"2406\", \"2407\", \"2408\",\n",
    "        \"2409\", \"2410\", \"2411\", \"2412\"\n",
    "    ]\n",
    "\n",
    "    for idx, file in enumerate(files):\n",
    "        print(f\"üîÑ Processing: {os.path.basename(file)}\")\n",
    "\n",
    "        ds = xr.open_dataset(file)\n",
    "        num_particles = ds.sizes['particles']\n",
    "\n",
    "        # Get group_id values\n",
    "        group_ids = ds['group_id'].values\n",
    "        release_time = release_times[idx]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({'group_id': group_ids})\n",
    "        df['group_index'] = df.groupby('group_id').cumcount()\n",
    "\n",
    "        # Create particle_id with padding\n",
    "        df['particle_id'] = df.apply(\n",
    "            lambda row: f\"{int(row['group_id']):03}{int(row['group_index']):02}{release_time}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Safety check\n",
    "        if len(df) != num_particles:\n",
    "            raise ValueError(f\"‚ùå Length mismatch in {file}: {len(df)} particle_ids vs {num_particles} particles\")\n",
    "\n",
    "        # Assign to dataset\n",
    "        ds['particle_id'] = (('particles'), df['particle_id'].values.astype('U15'))\n",
    "\n",
    "        # Save updated file\n",
    "        output_path = os.path.join(data_dir, f\"updated_{os.path.basename(file)}\")\n",
    "        ds.to_netcdf(output_path)\n",
    "        print(f\"‚úÖ Saved: {output_path}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/abolmaal/modelling/FVCOM/Huron/output/FVCOM_Huron_2424_JanFeb_1.nc', '/home/abolmaal/modelling/FVCOM/Huron/output/FVCOM_Huron_2424_FebMar_2.nc', '/home/abolmaal/modelling/FVCOM/Huron/output/FVCOM_Huron_2424_MarApr_3.nc', '/home/abolmaal/modelling/FVCOM/Huron/output/FVCOM_Huron_2424_AprMay_4.nc', '/home/abolmaal/modelling/FVCOM/Huron/output/FVCOM_Huron_2424_MayJun_5.nc', '/home/abolmaal/modelling/FVCOM/Huron/output/FVCOM_Huron_2424_JunJul_6.nc', '/home/abolmaal/modelling/FVCOM/Huron/output/FVCOM_Huron_2424_JulAug_7.nc', '/home/abolmaal/modelling/FVCOM/Huron/output/FVCOM_Huron_2424_AugSep_8.nc']\n"
     ]
    }
   ],
   "source": [
    "# Load the FVCOM output files and sort them based on the time\n",
    "files = glob.glob(FVCOM_dir + \"/FVCOM_Huron_2424*.nc\")\n",
    "files.sort(key=sort_key)\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing: FVCOM_Huron_2424_JanFeb_1.nc\n",
      "‚úÖ Saved: /home/abolmaal/modelling/FVCOM/Huron/output/updated_FVCOM_Huron_2424_JanFeb_1.nc\n",
      "\n",
      "üîÑ Processing: FVCOM_Huron_2424_FebMar_2.nc\n",
      "‚úÖ Saved: /home/abolmaal/modelling/FVCOM/Huron/output/updated_FVCOM_Huron_2424_FebMar_2.nc\n",
      "\n",
      "üîÑ Processing: FVCOM_Huron_2424_MarApr_3.nc\n",
      "‚úÖ Saved: /home/abolmaal/modelling/FVCOM/Huron/output/updated_FVCOM_Huron_2424_MarApr_3.nc\n",
      "\n",
      "üîÑ Processing: FVCOM_Huron_2424_AprMay_4.nc\n",
      "‚úÖ Saved: /home/abolmaal/modelling/FVCOM/Huron/output/updated_FVCOM_Huron_2424_AprMay_4.nc\n",
      "\n",
      "üîÑ Processing: FVCOM_Huron_2424_MayJun_5.nc\n",
      "‚úÖ Saved: /home/abolmaal/modelling/FVCOM/Huron/output/updated_FVCOM_Huron_2424_MayJun_5.nc\n",
      "\n",
      "üîÑ Processing: FVCOM_Huron_2424_JunJul_6.nc\n",
      "‚úÖ Saved: /home/abolmaal/modelling/FVCOM/Huron/output/updated_FVCOM_Huron_2424_JunJul_6.nc\n",
      "\n",
      "üîÑ Processing: FVCOM_Huron_2424_JulAug_7.nc\n",
      "‚úÖ Saved: /home/abolmaal/modelling/FVCOM/Huron/output/updated_FVCOM_Huron_2424_JulAug_7.nc\n",
      "\n",
      "üîÑ Processing: FVCOM_Huron_2424_AugSep_8.nc\n",
      "‚úÖ Saved: /home/abolmaal/modelling/FVCOM/Huron/output/updated_FVCOM_Huron_2424_AugSep_8.nc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to update the NetCDF files with the group_number column\n",
    "write_particle_id(files, FVCOM_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé First 10 particle_id values:\n",
      "['000002401' '000012401' '000022401' '000032401' '000042401' '000052401'\n",
      " '000062401' '000072401' '000082401' '000092401' '000102401' '000112401'\n",
      " '000122401' '000132401' '000142401' '000152401' '000162401' '000172401'\n",
      " '000182401' '000192401' '000202401' '000212401' '000222401' '000232401'\n",
      " '000242401' '000252401' '000262401' '000272401' '000282401' '000292401'\n",
      " '000302401' '000312401' '000322401' '000332401' '000342401' '000352401'\n",
      " '000362401' '000372401' '000382401' '000392401' '000402401' '000412401'\n",
      " '000422401' '000432401' '000442401' '000452401' '000462401' '000472401'\n",
      " '000482401' '000492401' '000502401' '000512401' '000522401' '000532401'\n",
      " '000542401' '000552401' '000562401' '000572401' '000582401' '000592401'\n",
      " '000602401' '000612401' '000622401' '000632401' '000642401' '000652401'\n",
      " '000662401' '000672401' '000682401' '000692401' '000702401' '000712401'\n",
      " '000722401' '000732401' '000742401' '003002401' '003012401' '003022401'\n",
      " '003032401' '003042401' '003052401' '003062401' '003072401' '003082401'\n",
      " '003092401' '003102401' '003112401' '003122401' '003132401' '003142401'\n",
      " '003152401' '003162401' '003172401' '003182401' '003192401' '003202401'\n",
      " '003212401' '003222401' '003232401' '003242401' '003252401' '003262401'\n",
      " '003272401' '003282401' '003292401' '003302401' '003312401' '003322401'\n",
      " '003332401' '003342401' '003352401' '003362401' '003372401' '003382401'\n",
      " '003392401' '003402401' '003412401' '003422401' '003432401' '003442401'\n",
      " '003452401' '003462401' '003472401' '003482401' '003492401' '003502401'\n",
      " '003512401' '003522401' '003532401' '003542401' '003552401' '003562401'\n",
      " '003572401' '003582401' '003592401' '003602401' '003612401' '003622401'\n",
      " '003632401' '003642401' '003652401' '003662401' '003672401' '003682401'\n",
      " '003692401' '003702401' '003712401' '003722401' '003732401' '003742401'\n",
      " '004002401' '004012401']\n"
     ]
    }
   ],
   "source": [
    "# Make sure the group_number column is added to the NetCDF files and is in right order \n",
    "# read the updated NetCDF file\n",
    "updated_files = glob.glob(FVCOM_dir + \"/updated_FVCOM_Huron_2424_*.nc\")\n",
    "updated_files.sort(key=sort_key)\n",
    "# Open the first file\n",
    "ds = xr.open_dataset(updated_files[0])\n",
    "\n",
    "# Print the first 10 particle_id values\n",
    "print(\"üîé First 10 particle_id values:\")\n",
    "print(ds['particle_id'].values[:152])\n",
    "# # read files usimg xarray\n",
    "# ds = xr.open_mfdataset(updated_files, combine='nested', concat_dim='time', parallel=True)\n",
    "\n",
    "# # read the firs file\n",
    "# #ds = xr.open_dataset(updated_files[:])\n",
    "# # print the updated group_number variable values\n",
    "# print(ds['group_number'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to read each file adn 1- # Step 1: Convert time to datetime (safe for large data)\n",
    "# Step 2: Extract day and month period\n",
    "# Step 3: Count unique days per month (memory-efficient)\n",
    "# Step_ 4: Create a DataFrame with the results\n",
    "def count_unique_days_per_month(files):\n",
    "    \"\"\"\n",
    "    Counts unique days per month across multiple NetCDF files.\n",
    "    Returns a DataFrame with the results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for file in files:\n",
    "        print(f\"üîÑ Processing: {os.path.basename(file)}\")\n",
    "        with xr.open_dataset(file) as ds:  # Use context manager\n",
    "\n",
    "            # Convert time to datetime\n",
    "            time = pd.to_datetime(ds['time'].values, unit='s', origin='unix')\n",
    "\n",
    "            # Extract day and month\n",
    "            days = time.day\n",
    "            months = time.month\n",
    "\n",
    "            # Create a DataFrame for counting unique days per month\n",
    "            df = pd.DataFrame({'day': days, 'month': months})\n",
    "            unique_days_per_month = df.groupby('month')['day'].nunique().reset_index()\n",
    "\n",
    "            # Append results\n",
    "            results.append(unique_days_per_month)\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    final_counts = final_df.groupby('month')['day'].sum().reset_index()\n",
    "    # save the final counts to a CSV file\n",
    "    output_csv = os.path.join(FVCOM_dir, 'days_per_releasetime_24.csv')\n",
    "    final_counts.to_csv(output_csv, index=False)\n",
    "    return final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing: updated_FVCOM_Huron_2424_JanFeb_1.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_FebMar_2.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_MarApr_3.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_AprMay_4.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_MayJun_5.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_JunJul_6.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_JulAug_7.nc\n",
      "üîÑ Processing: updated_FVCOM_Huron_2424_AugSep_8.nc\n"
     ]
    }
   ],
   "source": [
    "#  Call the function to count unique days per month\n",
    "unique_days_per_month = count_unique_days_per_month(updated_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the occurance of each group_id in ds using xarray\n",
    "group_id_counts = ds['group_id'].count(dim='particles').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year and month as 'YYYY-MM' from 'time'\n",
    "ds['month'] = ds['time'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = ds.to_dataframe()\n",
    "\n",
    "# Group by 'month' and 'group_id', and count the occurrences\n",
    "group_id_counts_df = df.groupby(['month', 'group_id']).size().reset_index(name='group_id_count')\n",
    "\n",
    "# Pivot the table to have months as rows and group_ids as columns\n",
    "pivot_df = group_id_counts_df.pivot(index='month', columns='group_id', values='group_id_count').fillna(0)\n",
    "\n",
    "# Print the resulting pivoted DataFrame\n",
    "print(pivot_df)\n",
    "#save this pivoted DataFrame to a csv file\n",
    "output_csv_path = os.path.join(FVCOM_dir, 'group_id_counts_by_month.csv')\n",
    "pivot_df.to_csv(output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a DataFrame for easier manipulation\n",
    "df = ds.to_dataframe().reset_index()  # Resetting the index to make 'time' a column\n",
    "\n",
    "# Check the first few rows to confirm the structure of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Filter the data for the specific day and group_id = 0\n",
    "filtered_df = df[(df['month'] == '2023-01') & (df['group_id'] == 0)]\n",
    "\n",
    "# Count the occurrences of group_id = 0 on the day 2023-01-01\n",
    "occurrences = filtered_df[filtered_df['time'] == '2023-01-01'].shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Occurrences of group_id=0 on 2023-01: {occurrences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a DataFrame for easier manipulation\n",
    "df = ds.to_dataframe().reset_index()  # Resetting the index to make 'time' a column\n",
    "\n",
    "# Check the first few rows to confirm the structure of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Filter the data for group_id = 0\n",
    "filtered_df = df[df['group_id'] == 0].copy()  # Use copy() to avoid warning\n",
    "\n",
    "# Convert the 'time' column to just the date (ignoring the time part)\n",
    "filtered_df['date'] = filtered_df['time'].dt.date  # Extract date without time\n",
    "\n",
    "# Extract month and year for filtering\n",
    "filtered_df['month'] = filtered_df['time'].dt.to_period('M')  # Use .loc to avoid warning\n",
    "\n",
    "# Filter data for the month of January 2023\n",
    "filtered_january = filtered_df[filtered_df['month'] == '2023-02']\n",
    "\n",
    "# Now count the total occurrences of group_id = 0 for the entire month of January 2023\n",
    "total_occurrences = filtered_january.shape[0]  # Count all rows\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total occurrences of group_id=0 in January 2023: {total_occurrences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NetCDF 'group_id' and 'group_number' variables to a DataFrame for processing\n",
    "netcdf_df = ds['group_id'].to_dataframe().reset_index()\n",
    "\n",
    "# Ensure that 'group_number' is extracted and correctly added\n",
    "netcdf_df['group_number'] = ds['group_number'].values\n",
    "\n",
    "# Step 1: Convert 'group_number' to integer if necessary, and format as a 5-digit string with leading zeros\n",
    "netcdf_df['group_number'] = netcdf_df['group_number'].apply(lambda x: f\"{int(x):11}\")\n",
    "\n",
    "# Step 2: Print values for debugging\n",
    "print(\"First few rows of netcdf_df:\")\n",
    "print(netcdf_df.head())\n",
    "\n",
    "# Step 3: Select relevant columns for saving to CSV\n",
    "netcdf_df = netcdf_df[['group_id', 'group_number']]\n",
    "\n",
    "# Save the result as a CSV file\n",
    "netcdf_df.to_csv(os.path.join(FVCOM_dir, 'group_id_group_number.csv'), index=False)\n",
    "\n",
    "print(\"CSV file saved with formatted group numbers\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
