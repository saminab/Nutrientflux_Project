{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: \n",
    "## Extract the path of particle tracking outputs\n",
    "Following code read the outputs of particle tracking and extracts the latitude and longitude "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1- Realease the particles (2_PT_seseflux)\n",
    "2- Read the particles for each month \n",
    "3- make a dataframe for particles and add group_number to the columns \n",
    "3- reduce to firs particles intersection to coastline, delete the ones that does not interact with coastline \n",
    "4- we make a dataframe that only include the particles first intersect with the shoreline for each month \n",
    "- another thing we need to add is to add a group_number to the particles in addition that group_id like 11, 12, 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1: Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from netCDF4 import Dataset\n",
    "from netCDF4 import MFDataset\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.colors import LinearSegmentedColormap, LogNorm\n",
    "import re \n",
    "import pylag\n",
    "from shapely.geometry import Point\n",
    "import contextily as ctx\n",
    "import datetime\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import gc \n",
    "import concurrent.futures\n",
    "import cftime  # Import cftime to handle cftime.DatetimeGregorian objects\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part1.1:Define Path to pylags output nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a path to the FVCOME data directory /home/samin/data/FVCOME_OUTPUT/Gldata/FVCOMEDATA\n",
    "# go forlder 202301 and red nc file contain *2230101.nc\n",
    "# define the path to the data directory\n",
    "data_dir = '/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(file):\n",
    "    filename = os.path.basename(file)\n",
    "    number = int(filename.split('__')[1].split('.')[0])\n",
    "    return number\n",
    "files = glob.glob(data_dir + \"/updated_Fvcome_huron_estuary_2023_Winter_*.nc\")\n",
    "files.sort(key=sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part1.2: Reading Outpufiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Jan__1.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Feb__2.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Mar__3.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Apr__4.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_May__5.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Jun__6.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_July__7.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Aug__8.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Sep__9.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Oct__10.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Nov__11.nc\n",
      "/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Dec__12.nc\n"
     ]
    }
   ],
   "source": [
    "def sort_key(file):\n",
    "    filename = os.path.basename(file)\n",
    "    number = int(filename.split('__')[1].split('.')[0])\n",
    "    return number\n",
    "\n",
    "# Get the output files and sort them using the sort_key function\n",
    "files = glob.glob(os.path.join(data_dir, 'updated_Fvcome_huron_estuary_2023_Winter_*.nc'))\n",
    "files.sort(key=sort_key)\n",
    "\n",
    "# Open multiple NetCDF datasets with chunks using xarray\n",
    "datasets = [xr.open_mfdataset(file) for file in files]\n",
    "\n",
    "# (Optional) Print the sorted file names to verify the sorting\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Extract variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2-1: Test to make a dataframe. Prepare Dataset and ensure we have the combined GeoDatafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the last output file in the dataset \n",
    "ds = datasets[-1]\n",
    "#get the particle id index \n",
    "group_id = ds['group_id'].values\n",
    "#get lat, lon and time for the last time step \n",
    "lat = ds['latitude'].values\n",
    "lon = ds['longitude'].values\n",
    "time = ds['time'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the variables for all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each dataset in the datasets list\n",
    "for dataset in datasets:\n",
    "    # Extract the required variables into a DataFrame\n",
    "    df = dataset[['latitude', 'longitude', 'time', 'group_id', 'WetLoad_TN_kg2', 'WetLoad_TP_kg2']].to_dataframe().reset_index()\n",
    "    \n",
    "    # If needed, adjust longitude\n",
    "    df.loc[df['longitude'] > 0, 'longitude'] = df.loc[df['longitude'] > 0, 'longitude'] - 360\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(dd.from_pandas(df, npartitions=10))  # Dask partitions for chunking\n",
    "\n",
    "# Concatenate all DataFrames into one using Dask\n",
    "combined_df = dd.concat(dataframes)\n",
    "\n",
    "# Compute the final DataFrame (brings it into memory in chunks)\n",
    "final_df = combined_df.compute()\n",
    "\n",
    "# Display the result\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data using nc Datasaete and creating dataframe and then geodataframe for each unique group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Jan__1.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Jan__1.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Feb__2.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Feb__2.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Mar__3.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Mar__3.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Apr__4.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Apr__4.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_May__5.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_May__5.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Jun__6.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Jun__6.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_July__7.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_July__7.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Aug__8.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Aug__8.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Sep__9.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Sep__9.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Oct__10.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Oct__10.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Nov__11.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Nov__11.nc returned no data.\n",
      "Error processing file /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Dec__12.nc: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4\n",
      "File /home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/updated_Fvcome_huron_estuary_2023_Winter_Dec__12.nc returned no data.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store DataFrames from each file\n",
    "df_list = []\n",
    "\n",
    "# Function to process each NetCDF file\n",
    "def process_file(file):\n",
    "    try:\n",
    "        # Step 1: Open the NetCDF file\n",
    "        dataset = nc.Dataset(file, 'r')\n",
    "\n",
    "        # Step 2: Ensure the necessary variables exist in the file\n",
    "        required_vars = ['latitude', 'longitude', 'time', 'group_id', 'WetLoad_TN_kg2', 'WetLoad_TP_kg2']\n",
    "        if not all(var in dataset.variables for var in required_vars):\n",
    "            print(f\"Skipping file {file}: Missing required variables.\")\n",
    "            return None\n",
    "\n",
    "        # Step 3: Extract the necessary variables and check their dimensions\n",
    "        latitude = dataset.variables['latitude'][:]\n",
    "        longitude = dataset.variables['longitude'][:]\n",
    "        time = dataset.variables['time'][:]\n",
    "        group_id = dataset.variables['group_id'][:]\n",
    "        wetload_tn = dataset.variables['WetLoad_TN_kg2'][:]\n",
    "        wetload_tp = dataset.variables['WetLoad_TP_kg2'][:]\n",
    "\n",
    "        # Diagnostic step: Print the unique group_ids for this file before any processing\n",
    "        unique_group_ids_file = set(group_id.flatten())  # Convert to set to get unique values\n",
    "        print(f\"File: {file} | Unique group_ids: {len(unique_group_ids_file)}\")\n",
    "\n",
    "        # Step 4: Convert the time variable to human-readable datetime format\n",
    "        time_units = dataset.variables['time'].units\n",
    "        time = nc.num2date(time, units=time_units)\n",
    "\n",
    "        # Handle cftime.DatetimeGregorian and convert to standard Python datetime objects where possible\n",
    "        time = pd.Series([t if isinstance(t, datetime) else t.strftime('%Y-%m-%d %H:%M:%S') for t in time], name='time')\n",
    "\n",
    "        # Step 5: Ensure all variables are 1-dimensional, flatten them if necessary\n",
    "        if latitude.ndim > 1:\n",
    "            latitude = latitude.flatten()\n",
    "        if longitude.ndim > 1:\n",
    "            longitude = longitude.flatten()\n",
    "        if group_id.ndim > 1:\n",
    "            group_id = group_id.flatten()\n",
    "        if wetload_tn.ndim > 1:\n",
    "            wetload_tn = wetload_tn.flatten()\n",
    "        if wetload_tp.ndim > 1:\n",
    "            wetload_tp = wetload_tp.flatten()\n",
    "\n",
    "        # # Ensure all arrays have the same length after flattening\n",
    "        min_length = min(len(latitude), len(longitude), len(time), len(group_id), len(wetload_tn), len(wetload_tp))\n",
    "        latitude = latitude[:min_length]\n",
    "        longitude = longitude[:min_length]\n",
    "        time = time[:min_length]\n",
    "        group_id = group_id[:min_length]\n",
    "        wetload_tn = wetload_tn[:min_length]\n",
    "        # wetload_tp = wetload_tp[:min_length]\n",
    "\n",
    "        # Step 6: Create a pandas DataFrame with the extracted data\n",
    "        df = pd.DataFrame({\n",
    "            'latitude': latitude,\n",
    "            'longitude': longitude,\n",
    "            'time': time,\n",
    "            'group_id': group_id,\n",
    "            'WetLoad_TN_kg2': wetload_tn,\n",
    "            'WetLoad_TP_kg2': wetload_tp\n",
    "        })\n",
    "\n",
    "        # Step 7: Adjust longitude values if necessary\n",
    "        df.loc[df['longitude'] > 0, 'longitude'] = df.loc[df['longitude'] > 0, 'longitude'] - 360\n",
    "\n",
    "        # Step 8: Create the geometry column using longitude and latitude\n",
    "        geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "        df['geometry'] = geometry\n",
    "\n",
    "        # Close the NetCDF dataset\n",
    "        dataset.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process all the files one by one\n",
    "for file in files:\n",
    "    df = process_file(file)\n",
    "    if df is not None:\n",
    "        df_list.append(df)\n",
    "    else:\n",
    "        print(f\"File {file} returned no data.\")\n",
    "\n",
    "    # Manually free memory\n",
    "    gc.collect()\n",
    "\n",
    "# Step 9: Check if there are any DataFrames in the list\n",
    "if len(df_list) > 0:\n",
    "    # Concatenate all DataFrames into one\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Debug: Check the number of unique group_ids extracted across all files\n",
    "    unique_group_ids = final_df['group_id'].unique()\n",
    "    print(f\"Total unique group_id values found across all files: {len(unique_group_ids)}\")\n",
    "    print(f\"Unique group_ids across all files: {unique_group_ids}\")\n",
    "\n",
    "    # Step 10: Convert the pandas DataFrame to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(final_df, geometry='geometry')\n",
    "\n",
    "    # Step 11: Set the Coordinate Reference System (CRS) to WGS 84 (EPSG:4326)\n",
    "    gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    # Step 12: Shorten column names to fit within Shapefile's 10-character limit\n",
    "    gdf.rename(columns={\n",
    "        'WetLoad_TN_kg2': 'TN_Load',\n",
    "        'WetLoad_TP_kg2': 'TP_Load',\n",
    "        'geometry': 'geom'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Step 13: Set the active geometry column to 'geom' after renaming\n",
    "    gdf = gdf.set_geometry('geom')\n",
    "\n",
    "    # Step 14: Convert the 'time' column to string format to avoid issues with datetime in shapefiles\n",
    "    gdf['time'] = gdf['time'].astype(str)\n",
    "\n",
    "    # Step 15: Check again for unique group_ids and verify data integrity\n",
    "    unique_group_ids = gdf['group_id'].unique()\n",
    "    print(f\"Total unique group_ids in GeoDataFrame: {len(unique_group_ids)}\")\n",
    "    print(f\"Unique group_ids in GeoDataFrame: {unique_group_ids}\")\n",
    "\n",
    "    # Step 16: Iterate over each unique group_id and create individual GeoDataFrames\n",
    "    for gid in unique_group_ids:\n",
    "        # Filter rows for the current group_id and select specific columns\n",
    "        subset = gdf[gdf['group_id'] == gid][['time','group_id','longitude', 'latitude', 'TN_Load', 'TP_Load','geom']]\n",
    "\n",
    "        # Create a GeoDataFrame for the current group_id\n",
    "        group_gdf = gpd.GeoDataFrame(subset, geometry='geom', crs=gdf.crs)\n",
    "\n",
    "        # Save each group GeoDataFrame as a shapefile or GeoJSON with shortened column names\n",
    "        #group_gdf.to_file(f\"group_{gid}_data.shp\")\n",
    "\n",
    "    print(\"Individual GeoDataFrames with geometry and shortened column names created successfully for each group_id.\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time  group_id  longitude   latitude       TN_Load  \\\n",
      "1330  2023-12-30 00:00:00         3 -84.230347  45.641350  1.767471e-08   \n",
      "1331  2023-12-30 06:00:00         3 -84.226349  45.641350  1.767471e-08   \n",
      "1332  2023-12-30 12:00:00         3 -84.242371  45.643349  1.767471e-08   \n",
      "1333  2023-12-30 18:00:00         3 -84.228363  45.639351  1.767471e-08   \n",
      "1334  2023-12-31 00:00:00         3 -84.238373  45.641350  1.767471e-08   \n",
      "\n",
      "           TP_Load                        geom  \n",
      "1330  1.156378e-09  POINT (-84.23035 45.64135)  \n",
      "1331  1.156378e-09  POINT (-84.22635 45.64135)  \n",
      "1332  1.156378e-09  POINT (-84.24237 45.64335)  \n",
      "1333  1.156378e-09  POINT (-84.22836 45.63935)  \n",
      "1334  1.156378e-09  POINT (-84.23837 45.64135)  \n"
     ]
    }
   ],
   "source": [
    "# print the lenght of the group_id in group_gdfs\n",
    "print (group_gdf.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2-2: Create a geodataframe from particle tracking outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a DataFrame\n",
    "df = datasets[['latitude', 'longitude', 'group_id', 'time','WetLoad_TN_kg2','WetLoad_TP_kg2']].to_dataframe().reset_index()\n",
    "\n",
    "# Ensure 'time' is in the DataFrame\n",
    "if 'time' not in df.columns:\n",
    "    raise KeyError(\"The 'time' column is missing from the DataFrame.\")\n",
    "# if lon > 0, then longitude = longitude - 360\n",
    "df.loc[df['longitude'] > 0, 'longitude'] -= 360\n",
    "# Inspect the DataFrame to check alignment\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty GeoDataFrame with time as the index\n",
    "time_index = pd.to_datetime(final_df['time'].unique())\n",
    "gdf = gpd.GeoDataFrame(index=time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store individual GeoDataFrames for each group_id\n",
    "gdfs = []\n",
    "\n",
    "unique_group_ids = df['group_id'].unique()\n",
    "\n",
    "for gid in unique_group_ids:\n",
    "    # Filter rows for the current group_id\n",
    "    subset = df[df['group_id'] == gid][['time', 'longitude', 'latitude', 'group_id']]\n",
    "    # if longitude is greater than > -0 convert it to longitude - 360\n",
    "    subset.loc[subset['longitude'] > -0, 'longitude'] = subset.loc[subset['longitude'] > -0, 'longitude'] - 360\n",
    "    # Ensure 'time' column is datetime\n",
    "    subset['time'] = pd.to_datetime(subset['time'])\n",
    "    subset['time'] = subset['time'].astype(str)\n",
    "    # Create a GeoDataFrame for the subset\n",
    "    subset_gdf = gpd.GeoDataFrame(subset, geometry=gpd.points_from_xy(subset.longitude, subset.latitude))\n",
    "    \n",
    "    # Append the subset GeoDataFrame to the list\n",
    "    gdfs.append(subset_gdf)\n",
    "\n",
    "# Combine all individual GeoDataFrames into one\n",
    "combined_gdf = pd.concat(gdfs)\n",
    "\n",
    "# Reset the index to time\n",
    "combined_gdf.set_index('time', inplace=True)\n",
    "\n",
    "# Ensure combined_gdf is correctly indexed and includes group_id\n",
    "combined_gdf['group_id'] = combined_gdf['group_id']\n",
    "\n",
    "# Set the GeoDataFrame's geometry\n",
    "combined_gdf['geometry'] = combined_gdf.apply(lambda row: row.geometry, axis=1)\n",
    "\n",
    "# Drop any columns that are not needed\n",
    "combined_gdf = combined_gdf.drop(columns=[col for col in combined_gdf.columns if col.startswith('geometry_') and col != 'geometry'])\n",
    "# convert datetime field to string \n",
    "#gdf['time'] = gdf.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "# Ensure the final GeoDataFrame is correct\n",
    "print(combined_gdf.tail(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2-3: save the geodataframe as a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the GeoDataFrame to a shapefile called combined_gdf_Jan.shp\n",
    "#combined_gdf_Feb = combined_gdf.copy()\n",
    "combined_gdf.to_file('/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/Extracted_latlongeodataframe_Pylag/particle_tracking_Jan.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the geodataframe to a shapefile\n",
    "combined_gdf.to_file('/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/Extracted_latlongeodataframe_Pylag/particle_tracking_Feb.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data frame to a csv file\n",
    "combined_gdf.to_csv('/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/Extracted_latlongeodataframe_Pylag/particle_tracking_Mar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part2-4: Plotting Particle Tracing versus Group ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and axis with a specified size\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 18))\n",
    "\n",
    "# Plot each group_id with a different color\n",
    "for gid in unique_group_ids:\n",
    "    subset = combined_gdf[combined_gdf['group_id'] == gid]\n",
    "    subset.plot(ax=ax, marker='>', label=f'Group {gid}', markersize=2)\n",
    "\n",
    "# Add a legend with a specified font size\n",
    "#plt.legend(fontsize=15)\n",
    "\n",
    "# Add title and labels with specified font sizes\n",
    "#plt.title('Particle Trajectories along the Huron Jan', fontsize=20)\n",
    "plt.xlabel('Longitude', fontsize=15)\n",
    "plt.ylabel('Latitude', fontsize=15)\n",
    "\n",
    "# Add grid and background color\n",
    "plt.grid(True)\n",
    "plt.gca().set_facecolor('grey')\n",
    "\n",
    "# Save the figure\n",
    "#plt.savefig('/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/results/particle_tracking_Jan.png')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18: Plot the data for visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "for gid in unique_group_ids:\n",
    "    subset = gdf[gdf['group_id'] == gid]\n",
    "    if not subset.empty:\n",
    "        subset.plot(ax=ax, marker='>', label=f'Group {gid}', markersize=2)\n",
    "    \n",
    "    # Set the aspect ratio manually\n",
    "ax.set_aspect('auto')\n",
    "\n",
    "    # Add a legend and labels\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('Particle Trajectories along the Huron Jan', fontsize=20)\n",
    "plt.xlabel('Longitude', fontsize=15)\n",
    "plt.ylabel('Latitude', fontsize=15)\n",
    "\n",
    "    # Show the plot\n",
    "plt.show()\n",
    "#else:\n",
    "    #print(\"No valid DataFrames were created from the NetCDF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, LogNorm\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as ccrs\n",
    "from pylag.processing.plot import FVCOMPlotter\n",
    "from pylag.processing.plot import create_figure, colourmap\n",
    "# Define a list of pink shades for the colormap\n",
    "pink_shades = ['#fff5f7', '#ffebf0', '#ffd6e1', '#ffbfd4', '#ff99c1', '#ff6ea9', '#ff4c92', '#ff2171', '#b50d4e']\n",
    "# Create a custom colormap\n",
    "pink_cmap = LinearSegmentedColormap.from_list('custom_pink', pink_shades)\n",
    "\n",
    "# Define a list of blue shades for the colormap and reverse it\n",
    "blue_shades = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594']\n",
    "blue_shades_reversed = blue_shades[::-1]\n",
    "blue_cmap_reversed = LinearSegmentedColormap.from_list('custom_blue_reversed', blue_shades_reversed)\n",
    "\n",
    "# Define a list of green shades for the colormap\n",
    "green_shades = ['#e8f5e9', '#c8e6c9', '#a5d6a7', '#81c784', '#66bb6a', '#4caf50', '#43a047', '#388e3c', '#2e7d32']\n",
    "# Create a custom colormap\n",
    "green_cmap = LinearSegmentedColormap.from_list('custom_green', green_shades)\n",
    "\n",
    "# Create a custom colormap\n",
    "blue_cmap_reversed = LinearSegmentedColormap.from_list('custom_blue_reversed', blue_shades_reversed)\n",
    "font_size = 15\n",
    "cmap = colourmap('h_r')\n",
    "\n",
    "# Create the figure and axis with FVCOM plotter\n",
    "fig, ax = create_figure(figure_size=(26.,26.),projection=ccrs.PlateCarree(), font_size=font_size, bg_color='gray')\n",
    "\n",
    "# Load bathymetry data\n",
    "grid_metrics_file_name = '/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/input/gridfile/grid_metrics_huron_senseflux_Seasonal_winter.nc'\n",
    "ds = Dataset(grid_metrics_file_name, 'r')\n",
    "bathy = -ds.variables['h'][:]\n",
    "ds.close()\n",
    "# Configure plotter\n",
    "plotter = FVCOMPlotter(grid_metrics_file_name,\n",
    "                       geographic_coords=True,\n",
    "                       font_size=font_size)\n",
    "\n",
    "# Plot bathymetry\n",
    "#extents = np.array([-84.10,-84.20, 45.58,45.65], dtype=float)\n",
    "#extensts = np.array([-84,-81.3, 43,46], dtype=float)\n",
    "#Lake Huron Ausable Point\n",
    "extents = np.array([275, 277.69, 43, 46.3], dtype=float)\n",
    "ax, plot = plotter.plot_field(ax, bathy, extents=extents, add_colour_bar=True, cb_label='Depth (m)',\n",
    "                              cmap=blue_cmap_reversed, zorder= 0, vmin=-60, vmax=0)\n",
    "\n",
    "# Overlay grid\n",
    "plotter.draw_grid(ax, linewidth=1.0)\n",
    "\n",
    "# Plot each group_id with a different color\n",
    "for gid in unique_group_ids:\n",
    "    subset = combined_gdf[combined_gdf['group_id'] == gid]\n",
    "    subset.plot( ax = ax, marker='<', label=f'Group {gid}', markersize=2, zorder=40)\n",
    "    plt.savefig('/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/results/particle_tracking_Jan.jpeg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: Find the Intersection of Coastal Wetland and Particle Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3-1: Read the shapefiles for coastal wetlands, Lake shore line buffer, particle tracking geo data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the shapefile\n",
    "data_dir = '/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/results/shapefiles'\n",
    "path = '/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output'\n",
    "#buffer_lh = gpd.read_file(os.path.join(data_dir, 'lh_shore_ESRI_100k_Buffer1000m_NAD1983.shp'))\n",
    "# I created the buffer shapefile with 240 meter mainly because littoral zone is around 200 meter from the shore and\n",
    "# we are mostly interested in the littoral zone\n",
    "# buffer shapefile\n",
    "buffer_lh = gpd.read_file(os.path.join(data_dir, 'lh_shore_ESRI_100k_Buffer240m_NAD1983_US.shp'))\n",
    "# coastal wetlands shapefile\n",
    "CW = gpd.read_file(os.path.join(data_dir, 'wetland_connected_avg_inundation_Albers.shp'))\n",
    "# particle tracking shapefile\n",
    "par_jan = gpd.read_file(os.path.join(path, 'combined_gdf_Jan.shp'))\n",
    "par_Feb = gpd.read_file(os.path.join(path, 'combined_gdf_Feb.shp'))\n",
    "par_Mar = gpd.read_file(os.path.join(path, 'combined_gdf_Mar.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the shapefile to a geodataframe\n",
    "buffer_gdf = gpd.GeoDataFrame(buffer_lh)\n",
    "CW_geo = gpd.GeoDataFrame(CW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the shapefile to a geodataframe\n",
    "combined_gdf_Jan = gpd.GeoDataFrame(par_jan)\n",
    "combined_gdf_Feb = gpd.GeoDataFrame(par_Feb)\n",
    "combined_gdf_Mar = gpd.GeoDataFrame(par_Mar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part3-2: Convert the geometry of shapefiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the geodataframe is the same coordinates which is 3174 for Great Lakes Albers\n",
    "# Set the original CRS of the GeoDataFrames because they particle tracking is lat/lon we first set it to 4326\n",
    "combined_gdf_Jan = combined_gdf_Jan.set_crs(epsg=4326) # particle data\n",
    "combined_gdf_Feb = combined_gdf_Feb.set_crs(epsg=4326) # particle data\n",
    "combined_gdf_Mar = combined_gdf_Mar.set_crs(epsg=4326) # particle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have the Great Lake Albers projection we re project it to Great lakes Albers\n",
    "combined_gdf_Jan = combined_gdf_Jan.to_crs(epsg=3174)\n",
    "combined_gdf_Feb = combined_gdf_Feb.to_crs(epsg=3174)\n",
    "combined_gdf_Mar = combined_gdf_Mar.to_crs(epsg=3174)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the CRS of CW_geo\n",
    "print(\"Coastal wetland\", CW_geo.crs)\n",
    "print (\"Buffer\", buffer_gdf.crs)\n",
    "print(\"combined_gdf_Jan\", combined_gdf_Jan.crs)\n",
    "print(\"combined_gdf_Feb\", combined_gdf_Feb.crs)\n",
    "print(\"combined_gdf_Mar\", combined_gdf_Mar.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3-3: Reading Grid file from Pylag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#data_dir = '/home/samin/data/FVCOME_OUTPUT/Gldata/FVCOMEDATA/202301'.format(os.environ['HOME'])\n",
    "data_dir='/home/abolmaal/data/FVCOME_OUTPUT/Gldata/FVCOMEDATA/202301'.format(os.environ['HOME']) \n",
    "# Create run directory\n",
    "cwd = os.getcwd()\n",
    "# Create run directory\n",
    "simulation_dir = '/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron'.format(cwd)\n",
    "try:\n",
    "    os.makedirs(simulation_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Create input sub-directory\n",
    "input_dir = '{}/input'.format(simulation_dir)\n",
    "try:\n",
    "    os.makedirs(input_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "grid_metrics_file_name = f'{input_dir}/gridfile/grid_metrics_huron_senseflux_Seasonal_winter.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3-4: Find the intersection of polygon and Point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Following section we will use geopandas spatial join to find the intersection for the particle tracking out put for:\n",
    "1- between the lh_shore_ESRI_100k_Buffer1000m_WGS84 and particle tracking\n",
    "2- Between wetland_connected_avg_Buffer (which is a 500 meter buffer for the coastal wetland) with Particle tracking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the average number of trajectories entering the shore line for the fisrt three month of Winter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the average particles enter each polygone for the fisrt time calculate the average number of particles enter to the shoreline buffer and print how many percent of particles from the total trajectories remain in the shoreline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " to calculate how many of all the points first interact with the shoreline, we need to consider all individual points, not just those grouped by group_id. We'll determine the earliest interaction for each point within the points_within_shoreline DataFrame, and then calculate how many of these points first interact with the shoreline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, LogNorm\n",
    "\n",
    "# Load your monthly GeoDataFrames\n",
    "monthly_data = {\n",
    "    'Jan': combined_gdf_Jan,\n",
    "    'Feb': combined_gdf_Feb,\n",
    "    'Mar': combined_gdf_Mar,\n",
    "    # Add more months as needed\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# DataFrame to store particle counts for each polygon across all months\n",
    "all_particle_counts = pd.DataFrame(columns=['index_right', 'count'])\n",
    "\n",
    "# Define a list of blue shades for the colormap\n",
    "blue_shades = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594']\n",
    "\n",
    "# Create a custom colormap\n",
    "blue_cmap = LinearSegmentedColormap.from_list('custom_blue', blue_shades)\n",
    "\n",
    "total_particles_intersecting_buffer = 0\n",
    "total_points_first_interaction = 0\n",
    "\n",
    "# Iterate over each month's data\n",
    "for month, combined_gdf in monthly_data.items():\n",
    "    # Ensure 'time' is a column and convert to datetime\n",
    "    if 'time' in combined_gdf.index.names:\n",
    "        combined_gdf.reset_index(inplace=True)\n",
    "    combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])\n",
    "    \n",
    "    # Perform spatial join to find which points fall within the polygons\n",
    "    points_within_shoreline = gpd.sjoin(combined_gdf, buffer_gdf[['geometry']], how='inner', predicate='intersects')\n",
    "    \n",
    "    # Ensure 'time' is a column in the intersections and convert to datetime\n",
    "    if 'time' in points_within_shoreline.index.names:\n",
    "        points_within_shoreline.reset_index(inplace=True)\n",
    "    points_within_shoreline['time'] = pd.to_datetime(points_within_shoreline['time'])\n",
    "    \n",
    "    # Sort the intersection by time to find the first intersection for all particles\n",
    "    points_within_shoreline = points_within_shoreline.sort_values(by='time')\n",
    "    \n",
    "    # Calculate the total number of points that intersect the shoreline buffer\n",
    "    total_points_first_interaction += points_within_shoreline.drop_duplicates(subset=['geometry']).shape[0]\n",
    "    \n",
    "    # Calculate the total number of particles intersecting the buffer\n",
    "    total_particles_intersecting_buffer += points_within_shoreline['group_id'].nunique()\n",
    "    \n",
    "# Calculate the average number of particles entering the shoreline buffer\n",
    "average_particles_entering_buffer = total_particles_intersecting_buffer / len(monthly_data)\n",
    "\n",
    "# Calculate the percentage of points that first interact with the shoreline buffer\n",
    "#percentage_points_first_interaction = (total_points_first_interaction / total_particles_intersecting_buffer) \n",
    "\n",
    "print(f'Average number of particles entering the shoreline buffer: {average_particles_entering_buffer}')\n",
    "print(f'Percentage of points that first interact with the shoreline buffer: {percentage_points_first_interaction:.2f}%')\n",
    "\n",
    "# Plot the average number of particles entering the shoreline buffer\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# Create the hexbin plot\n",
    "hb = ax.hexbin(points_within_shoreline['longitude'], points_within_shoreline['latitude'], gridsize=100, cmap=blue_cmap, norm=LogNorm())\n",
    "\n",
    "# Set the title of the colormap to the average number of particles that fall within the polygons\n",
    "ax.set_title('Average number of trajectories returning to shoreline over the winter of 2023')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(hb)\n",
    "cbar.set_label('Average Number of Trajectories')\n",
    "plt.gca().set_facecolor('lightsteelblue')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, LogNorm\n",
    "import numpy as np\n",
    "from pylag.processing.plot import FVCOMPlotter, create_figure, colourmap\n",
    "import cartopy.crs as ccrs\n",
    "from netCDF4 import Dataset\n",
    "from pylag.processing.utils import get_grid_bands\n",
    "\n",
    "# Load your monthly GeoDataFrames\n",
    "monthly_data = {\n",
    "    'Jan': combined_gdf_Jan,\n",
    "    'Feb': combined_gdf_Feb,\n",
    "    'Mar': combined_gdf_Mar,\n",
    "    # Add more months as needed\n",
    "}\n",
    "\n",
    "\n",
    "# Ensure all GeoDataFrames use the same CRS\n",
    "buffer_gdf_crs = buffer_gdf.crs\n",
    "\n",
    "# DataFrame to store particle counts for each polygon across all months\n",
    "all_particle_counts = pd.DataFrame(columns=['index_right', 'count'])\n",
    "\n",
    "# Define a list of pink shades for the colormap\n",
    "pink_shades = ['#fff5f7', '#ffebf0', '#ffd6e1', '#ffbfd4', '#ff99c1', '#ff6ea9', '#ff4c92', '#ff2171', '#b50d4e']\n",
    "# Create a custom colormap\n",
    "pink_cmap = LinearSegmentedColormap.from_list('custom_pink', pink_shades)\n",
    "\n",
    "# Define a list of blue shades for the colormap and reverse it\n",
    "blue_shades = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594']\n",
    "blue_shades_reversed = blue_shades[::-1]\n",
    "# Create a custom colormap\n",
    "blue_cmap_reversed = LinearSegmentedColormap.from_list('custom_blue_reversed', blue_shades_reversed)\n",
    "\n",
    "\n",
    "total_particles_intersecting_buffer = 0\n",
    "total_particles_tracked = 0\n",
    "points_within_shoreline_list = []\n",
    "total_particles_list = []\n",
    "\n",
    "# Store the hexbin data for each month to calculate the average\n",
    "all_bin_counts = []\n",
    "\n",
    "# Collect all coordinates for a consistent hexbin grid\n",
    "all_coords = []\n",
    "\n",
    "# Iterate over each month's data\n",
    "for month, combined_gdf in monthly_data.items():\n",
    "    # Ensure 'time' is a column and convert to datetime\n",
    "    if 'time' in combined_gdf.index.names:\n",
    "        combined_gdf.reset_index(inplace=True)\n",
    "    combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])\n",
    "    \n",
    "    # Perform spatial join to find which points fall within the coastal buffer\n",
    "    points_within_shoreline = gpd.sjoin(combined_gdf, buffer_gdf[['geometry']], how='inner', predicate='intersects')\n",
    "    \n",
    "    # Ensure 'time' is a column in the intersections and convert to datetime\n",
    "    if 'time' in points_within_shoreline.index.names:\n",
    "        points_within_shoreline.reset_index(inplace=True)\n",
    "    points_within_shoreline['time'] = pd.to_datetime(points_within_shoreline['time'])\n",
    "    \n",
    "    # Sort the intersection by time to find the first intersection for all particles\n",
    "    points_within_shoreline = points_within_shoreline.sort_values(by='time')\n",
    "    \n",
    "    # Create a GeoDataFrame for the first intersections using the same CRS\n",
    "    geometry = gpd.points_from_xy(points_within_shoreline['longitude'], points_within_shoreline['latitude'], crs=buffer_gdf_crs)\n",
    "    first_intersections_shoreline = gpd.GeoDataFrame(points_within_shoreline, geometry=geometry, crs=buffer_gdf_crs)\n",
    "    \n",
    "    # Collect coordinates for hexbin grid\n",
    "    all_coords.extend(zip(first_intersections_shoreline.geometry.x, first_intersections_shoreline.geometry.y))\n",
    "\n",
    "font_size = 15\n",
    "cmap = colourmap('h_r')\n",
    "\n",
    "# Create the figure and axis with FVCOM plotter\n",
    "fig, ax = create_figure(figure_size=(26., 26.), projection=ccrs.PlateCarree(), font_size=font_size, bg_color='gray')\n",
    "\n",
    "# Load bathymetry data\n",
    "\n",
    "ds = Dataset(grid_metrics_file_name, 'r')\n",
    "bathy = -ds.variables['h'][:]\n",
    "ds.close()\n",
    "\n",
    "# Configure plotter\n",
    "plotter = FVCOMPlotter(grid_metrics_file_name, geographic_coords=True, font_size=font_size)\n",
    "extents = np.array([275, 277.69, 43, 46.3], dtype=float )\n",
    "ax, plot = plotter.plot_field(ax, bathy, extents=extents, add_colour_bar=True, cb_label='Depth (m)', vmin=-60., vmax=0., cmap=blue_cmap_reversed, zorder= 0)\n",
    "\n",
    "# Overlay grid= \n",
    "plotter.draw_grid(ax, linewidth=1.0)\n",
    "\n",
    "# Create a hexbin plot to get the consistent grid\n",
    "hb = ax.hexbin(*zip(*all_coords), gridsize=100, cmap=pink_cmap, norm=LogNorm())\n",
    "hexbin_grid = hb.get_offsets()\n",
    "\n",
    "# Initialize an array to accumulate bin counts\n",
    "accumulated_bin_counts = np.zeros(len(hexbin_grid))\n",
    "\n",
    "# Iterate over each month's data again to accumulate counts\n",
    "for month, combined_gdf in monthly_data.items():\n",
    "    # Ensure 'time' is a column and convert to datetime\n",
    "    if 'time' in combined_gdf.index.names:\n",
    "        combined_gdf.reset_index(inplace=True)\n",
    "    combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])\n",
    "    \n",
    "    # Perform spatial join to find which points fall within the coastal buffer\n",
    "    points_within_shoreline = gpd.sjoin(combined_gdf, buffer_gdf[['geometry']], how='inner', predicate='intersects')\n",
    "    \n",
    "    # Ensure 'time' is a column in the intersections and convert to datetime\n",
    "    if 'time' in points_within_shoreline.index.names:\n",
    "        points_within_shoreline.reset_index(inplace=True)\n",
    "    points_within_shoreline['time'] = pd.to_datetime(points_within_shoreline['time'])\n",
    "    \n",
    "    # Sort the intersection by time to find the first intersection for all particles\n",
    "    points_within_shoreline = points_within_shoreline.sort_values(by='time')\n",
    "    \n",
    "    # Create a GeoDataFrame for the first intersections using the same CRS\n",
    "    geometry = gpd.points_from_xy(points_within_shoreline['longitude'], points_within_shoreline['latitude'], crs=buffer_gdf_crs)\n",
    "    first_intersections_shoreline = gpd.GeoDataFrame(points_within_shoreline, geometry=geometry, crs=buffer_gdf_crs)\n",
    "    \n",
    "    # Create a hexbin plot with the consistent grid and accumulate counts\n",
    "    hb = ax.hexbin(first_intersections_shoreline.geometry.x, first_intersections_shoreline.geometry.y, gridsize=100, cmap=pink_cmap, norm=LogNorm(), reduce_C_function=np.sum, zorder=40)\n",
    "    bin_counts = hb.get_array()\n",
    "    for i, count in enumerate(bin_counts):\n",
    "        accumulated_bin_counts[i] += count\n",
    "    \n",
    "    # Calculate the total number of particles that intersect the shoreline buffer\n",
    "    total_particles_intersecting_buffer += first_intersections_shoreline['group_id'].nunique()\n",
    "    \n",
    "    # Calculate the total number of particles tracked in the month\n",
    "    total_particles_tracked += combined_gdf['group_id'].nunique()\n",
    "    total_particles_list.append(combined_gdf.shape[0])\n",
    "    points_within_shoreline_list.append(first_intersections_shoreline.shape[0])\n",
    "\n",
    "# Calculate the average bin counts\n",
    "average_bin_counts = accumulated_bin_counts / len(monthly_data)\n",
    "\n",
    "# Calculate the average number of particles entering the shoreline buffer\n",
    "average_particles_entering_shoreline = total_particles_intersecting_buffer / len(monthly_data)\n",
    "\n",
    "# Calculate the percentage of particles from the total trajectories that remain in the shoreline buffer\n",
    "percentage_particles_remain_in_shoreline = (total_particles_intersecting_buffer / total_particles_tracked) * 100\n",
    "\n",
    "print(f'Average number of particles entering the shoreline: {average_particles_entering_shoreline}')\n",
    "print(f'Percentage of particles from total trajectories that remain in the shoreline: {percentage_particles_remain_in_shoreline:.2f}%')\n",
    "\n",
    "# Update the hexbin plot with average bin counts\n",
    "hb = ax.hexbin(*zip(*hexbin_grid), gridsize=100, C=average_bin_counts, cmap=pink_cmap, norm=LogNorm())\n",
    "\n",
    "# Print the average number of particles in each bin\n",
    "print(\"Average number of particles in each bin:\")\n",
    "for i, count in enumerate(average_bin_counts):\n",
    "    print(f\"Bin {i}: {count:.2f} particles\")\n",
    "\n",
    "# Set the title of the colormap to the average number of particles that fall within the polygons\n",
    "#ax.set_title('Average number of particles returning to the shoreline over Winter 2023')\n",
    "\n",
    "# Add x and y labels\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(hb, ax=ax, pad=0.09)\n",
    "cbar.set_label('Average Number of Particles in each bin', fontsize=15)\n",
    "\n",
    "# Adjust layout to make space for colorbar\n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3-3-1: In this part we are going to calculate when the particles first hit the shoreline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the crs of ds\n",
    "ds = Dataset(grid_metrics_file_name, 'r')\n",
    "bathy = -ds.variables['h'][:]\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_metrics_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the points_within_shoreline columns\n",
    "print(points_within_shoreline.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort intersections by group_id and time\n",
    "points_within_shoreline = points_within_shoreline.sort_values(by=['group_id', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by group_id and then find the fosrt intersection for each group \n",
    "# this willl keep only thr fisrt intersection for each group at each time step\n",
    "first_intersections = points_within_shoreline.groupby(['group_id','time']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the earliest_intersection to a csv file\n",
    "first_intersections.to_csv('/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/output/earliest_intersections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the points_within_shoreline with group_id 0\n",
    "points_within_shoreline[points_within_shoreline['group_id'] == 2]\n",
    "# Plot the points_within_shoreline with group_id 0\n",
    "points_within_shoreline[points_within_shoreline['group_id'] == 2].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of points that fall within the polygons\n",
    "num_points_within_shoreline = points_within_shoreline\n",
    "\n",
    "# Print the resulting GeoDataFrame\n",
    "print(f\"Number of points within shoreline: {num_points_within_shoreline.shape[0]}\")\n",
    "# print the total number of points from the combined_gdf\n",
    "print(f\"Total number of particle points: {combined_gdf.shape[0]}\")\n",
    "print(f\"points within shoreline: {num_points_within_shoreline}\")\n",
    "\n",
    "# print the combined_gdf group_id that fall within the polygons with Watershed\n",
    "print(points_within_shoreline[['longitude', 'latitude', 'group_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of points that fall within the polygons\n",
    "num_points_within_shoreline = earliest_intersection\n",
    "\n",
    "# Print the resulting GeoDataFrame\n",
    "print(f\"Number of points within shoreline: {num_points_within_shoreline.shape[0]}\")\n",
    "# print the total number of points from the combined_gdf\n",
    "print(f\"Total number of particle points: {combined_gdf.shape[0]}\")\n",
    "print(f\"points within shoreline: {num_points_within_shoreline}\")\n",
    "\n",
    "# print the combined_gdf group_id that fall within the polygons with Watershed\n",
    "print(points_within_shoreline[['longitude', 'latitude', 'group_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a bar chart that shows the number of points that fall within the polygons and the total number of points\n",
    "# Create a bar chart and make the number of points log scale\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "plt.bar(['Total Trajectories', 'Trajectories within Shoreline'], [combined_gdf.shape[0], num_points_within_shoreline.shape[0]])\n",
    "plt.ylabel('Total number of trajectories')\n",
    "plt.title('Number of trajectories within Shoreline Jan 2023')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a seaborn  plot showing all the data points vesus the points that fall within the polygons\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n",
    "sns.scatterplot(data=combined_gdf, x='longitude', y='latitude', hue='group_id', palette='tab20', ax=ax)\n",
    "sns.scatterplot(data=points_within_shoreline, x='longitude', y='latitude', hue='group_id', palette='tab20', ax=ax, marker='x', s=100)\n",
    "\n",
    "# Add a title\n",
    "ax.set_title('DParticle tracking Points vs Points Within Polygons')\n",
    "\n",
    "# Adjust the legend to have two columns\n",
    "plt.legend(loc='upper left', ncol=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 3-5: Finding the intersection of particle tracking and coastal Wetlands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Great lake shapefile \n",
    "#basin = gpd.read_file('/home/abolmaal/data/FVCOME_OUTPUT/shapefiles/Basins/glbasins_gen.shp')\n",
    "lake = gpd.read_file('/home/abolmaal/data/FVCOME_OUTPUT/shapefiles/Basins/hydro_p_LakeHuron/hydro_p_LakeHuron_84.shp')\n",
    "# print the basin and lake crs\n",
    "#print(basin.crs)\n",
    "print(lake.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, LogNorm\n",
    "\n",
    "# Load your monthly GeoDataFrames\n",
    "monthly_data = {\n",
    "    'Jan': combined_gdf_Jan,\n",
    "    'Feb': combined_gdf_Feb,\n",
    "    'Mar': combined_gdf_Mar,\n",
    "    # Add more months as needed\n",
    "}\n",
    "\n",
    "# Coastal wetland data\n",
    "\n",
    "\n",
    "# DataFrame to store particle counts for each polygon across all months\n",
    "all_particle_counts = pd.DataFrame(columns=['index_right', 'count'])\n",
    "\n",
    "# Define a list of blue shades for the colormap\n",
    "blue_shades = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594']\n",
    "# Create a custom colormap\n",
    "blue_cmap = LinearSegmentedColormap.from_list('custom_blue', blue_shades)\n",
    "\n",
    "total_particles_intersecting_CW = 0\n",
    "total_particles_tracked = 0\n",
    "points_within_CW_list = []\n",
    "total_particles_list = []\n",
    "\n",
    "# Iterate over each month's data\n",
    "for month, combined_gdf in monthly_data.items():\n",
    "    # Ensure 'time' is a column and convert to datetime\n",
    "    if 'time' in combined_gdf.index.names:\n",
    "        combined_gdf.reset_index(inplace=True)\n",
    "    combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])\n",
    "    \n",
    "    # Perform spatial join to find which points fall within the coastal wetland\n",
    "    points_within_CW = gpd.sjoin(combined_gdf, CW_geo[['geometry']], how='inner', predicate='intersects')\n",
    "    \n",
    "    # Ensure 'time' is a column in the intersections and convert to datetime\n",
    "    if 'time' in points_within_CW.index.names:\n",
    "        points_within_CW.reset_index(inplace=True)\n",
    "    points_within_CW['time'] = pd.to_datetime(points_within_CW['time'])\n",
    "    \n",
    "    # Sort the intersection by time to find the first intersection for all particles\n",
    "    points_within_CW= points_within_CW.sort_values(by='time')\n",
    "    \n",
    "    # Get the first intersection for all particles\n",
    "    first_intersections_CW = points_within_CW.groupby('geometry').first().reset_index()\n",
    "    \n",
    "    # Count the number of particles entering each polygon (shoreline area)\n",
    "    particle_counts = first_intersections_CW['index_right'].value_counts().reset_index()\n",
    "    particle_counts.columns = ['index_right', 'count']\n",
    "    \n",
    "    # Calculate the total number of particles that intersect the shoreline buffer\n",
    "    total_particles_intersecting_CW += first_intersections_CW['group_id'].nunique()\n",
    "    \n",
    "    # Calculate the total number of particles tracked in the month\n",
    "    total_particles_tracked += combined_gdf['group_id'].nunique()\n",
    "    total_particles_list.append(combined_gdf.shape[0])\n",
    "    points_within_CW_list.append(first_intersections_CW.shape[0])\n",
    "# Calculate the average number of particles entering the shoreline buffer\n",
    "average_particles_entering_CW = total_particles_intersecting_CW / len(monthly_data)\n",
    "\n",
    "# Calculate the percentage of particles from the total trajectories that remain in the shoreline buffer\n",
    "percentage_particles_remain_in_CW = (total_particles_intersecting_CW / total_particles_tracked) \n",
    "\n",
    "print(f'Average number of particles entering the Coastal Wetland: {average_particles_entering_CW}')\n",
    "print(f'Percentage of particles from total trajectories that remain in the Coastal Wetland: {percentage_particles_remain_in_CW:.2f}%')\n",
    "\n",
    "\n",
    "# Plot the average number of particles entering the coastal wetland\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Create the hexbin plot\n",
    "hb = ax.hexbin(first_intersections_CW['longitude'], first_intersections_CW['latitude'], gridsize=30, cmap=blue_cmap, norm=LogNorm())\n",
    "#plot the buffer_gdf\n",
    "#buffer_gdf.boundary.plot(ax=ax, edgecolor='black')\n",
    "# plot the lake_gdf based on latitude and longitude in the geoDataFrame\n",
    "  \n",
    "# Set the title of the colormap to the average number of particles that fall within the polygons\n",
    "ax.set_title('Average number of particles returning to the coastal wetland over Winter 2023')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(hb)\n",
    "cbar.set_label('Average Number of Particles')\n",
    "plt.gca().set_facecolor('gainsboro')\n",
    "plt.savefig('/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/results/average_number_of_particles_entering_coastal_wetland.png')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#data_dir = '/home/samin/data/FVCOME_OUTPUT/Gldata/FVCOMEDATA/202301'.format(os.environ['HOME'])\n",
    "data_dir='/home/abolmaal/data/FVCOME_OUTPUT/Gldata/FVCOMEDATA/202301'.format(os.environ['HOME']) \n",
    "# Create run directory\n",
    "cwd = os.getcwd()\n",
    "# Create run directory\n",
    "simulation_dir = '/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron'.format(cwd)\n",
    "try:\n",
    "    os.makedirs(simulation_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Create input sub-directory\n",
    "input_dir = '{}/input'.format(simulation_dir)\n",
    "try:\n",
    "    os.makedirs(input_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "grid_metrics_file_name = f'{input_dir}/gridfile/grid_metrics_huron_senseflux_Seasonal_winter.nc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, LogNorm\n",
    "import numpy as np\n",
    "from pylag.processing.plot import FVCOMPlotter\n",
    "from pylag.processing.plot import create_figure, colourmap\n",
    "import cartopy.crs as ccrs\n",
    "from netCDF4 import Dataset\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from pylag.regrid import regridder\n",
    "from pylag.processing.ncview import Viewer\n",
    "from pylag.processing.plot import FVCOMPlotter\n",
    "from pylag.processing.plot import create_figure\n",
    "from pylag.processing.utils import get_grid_bands\n",
    "\n",
    "# Load your monthly GeoDataFrames\n",
    "monthly_data = {\n",
    "    'Jan': combined_gdf_Jan,\n",
    "    'Feb': combined_gdf_Feb,\n",
    "    'Mar': combined_gdf_Mar,\n",
    "    # Add more months as needed\n",
    "}\n",
    "# Ensure all GeoDataFrames use the same CRS\n",
    "CW_geo_crs = CW_geo.crs\n",
    "\n",
    "\n",
    "# DataFrame to store particle counts for each polygon across all months\n",
    "all_particle_counts = pd.DataFrame(columns=['index_right', 'count'])\n",
    "\n",
    "# Define a list of pink shades for the colormap\n",
    "pink_shades = ['#fff5f7', '#ffebf0', '#ffd6e1', '#ffbfd4', '#ff99c1', '#ff6ea9', '#ff4c92', '#ff2171', '#b50d4e']\n",
    "# Create a custom colormap\n",
    "pink_cmap = LinearSegmentedColormap.from_list('custom_pink', pink_shades)\n",
    "# Define a list of blue shades for the colormap\n",
    "# Define a list of blue shades for the colormap\n",
    "# Define a list of blue shades for the colormap and reverse it\n",
    "# Define a list of blue shades for the colormap and reverse it\n",
    "blue_shades = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594']\n",
    "blue_shades_reversed = blue_shades[::-1]\n",
    "# Create a custom colormap\n",
    "blue_cmap_reversed = LinearSegmentedColormap.from_list('custom_blue_reversed', blue_shades_reversed)\n",
    "\n",
    "\n",
    "\n",
    "total_particles_intersecting_CW = 0\n",
    "total_particles_tracked = 0\n",
    "points_within_CW_list = []\n",
    "total_particles_list = []\n",
    "\n",
    "# Store the hexbin data for each month to calculate the average\n",
    "all_bin_counts = []\n",
    "\n",
    "# Collect all coordinates for a consistent hexbin grid\n",
    "all_coords = []\n",
    "\n",
    "# Iterate over each month's data\n",
    "for month, combined_gdf in monthly_data.items():\n",
    "    # Ensure 'time' is a column and convert to datetime\n",
    "    if 'time' in combined_gdf.index.names:\n",
    "        combined_gdf.reset_index(inplace=True)\n",
    "    combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])\n",
    "    \n",
    "    # Perform spatial join to find which points fall within the coastal wetland\n",
    "    points_within_CW = gpd.sjoin(combined_gdf, CW_geo[['geometry']], how='inner', predicate='intersects')\n",
    "    \n",
    "    # Ensure 'time' is a column in the intersections and convert to datetime\n",
    "    if 'time' in points_within_CW.index.names:\n",
    "        points_within_CW.reset_index(inplace=True)\n",
    "    points_within_CW['time'] = pd.to_datetime(points_within_CW['time'])\n",
    "    \n",
    "    # Sort the intersection by time to find the first intersection for all particles\n",
    "    points_within_CW = points_within_CW.sort_values(by='time')\n",
    "    \n",
    "    # Create a GeoDataFrame for the first intersections using the same CRS\n",
    "    geometry = gpd.points_from_xy(points_within_CW['longitude'], points_within_CW['latitude'], crs=CW_geo_crs)\n",
    "    first_intersections_CW = gpd.GeoDataFrame(points_within_CW, geometry=geometry, crs=CW_geo_crs)\n",
    "    \n",
    "    # Collect coordinates for hexbin grid\n",
    "    all_coords.extend(zip(first_intersections_CW.geometry.x, first_intersections_CW.geometry.y))\n",
    "\n",
    "font_size = 15\n",
    "cmap = colourmap('h_r')\n",
    "\n",
    "# Create the figure and axis with FVCOM plotter\n",
    "fig, ax = create_figure(figure_size=(26., 26.), projection=ccrs.PlateCarree(), font_size=font_size, bg_color='gray')\n",
    "\n",
    "# Load bathymetry data\n",
    "grid_metrics_file_name = '/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/input/gridfile/grid_metrics_huron_senseflux_Seasonal_winter.nc'\n",
    "\n",
    "ds = Dataset(grid_metrics_file_name, 'r')\n",
    "bathy = -ds.variables['h'][:]\n",
    "ds.close()\n",
    "\n",
    "# Configure plotter\n",
    "plotter = FVCOMPlotter(grid_metrics_file_name, geographic_coords=True, font_size=font_size)\n",
    "extents = np.array([275, 277.69, 43, 46.3], dtype=float)\n",
    "ax, plot = plotter.plot_field(ax, bathy, extents=extents, add_colour_bar=True, cb_label='Depth (m)', vmin=-60., vmax=0., cmap=blue_cmap_reversed,zorder=0)\n",
    "\n",
    "# Overlay grid\n",
    "plotter.draw_grid(ax, linewidth=1.0)\n",
    "\n",
    "# Create a hexbin plot to get the consistent grid\n",
    "hb = ax.hexbin(*zip(*all_coords), gridsize=100, cmap=pink_cmap, norm=LogNorm())\n",
    "hexbin_grid = hb.get_offsets()\n",
    "\n",
    "# Initialize an array to accumulate bin counts\n",
    "accumulated_bin_counts = np.zeros(len(hexbin_grid))\n",
    "\n",
    "# Iterate over each month's data again to accumulate counts\n",
    "for month, combined_gdf in monthly_data.items():\n",
    "    # Ensure 'time' is a column and convert to datetime\n",
    "    if 'time' in combined_gdf.index.names:\n",
    "        combined_gdf.reset_index(inplace=True)\n",
    "    combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])\n",
    "    \n",
    "    # Perform spatial join to find which points fall within the coastal wetland\n",
    "    points_within_CW = gpd.sjoin(combined_gdf, CW_geo[['geometry']], how='inner', predicate='intersects')\n",
    "    \n",
    "    # Ensure 'time' is a column in the intersections and convert to datetime\n",
    "    if 'time' in points_within_CW.index.names:\n",
    "        points_within_CW.reset_index(inplace=True)\n",
    "    points_within_CW['time'] = pd.to_datetime(points_within_CW['time'])\n",
    "    \n",
    "    # Sort the intersection by time to find the first intersection for all particles\n",
    "    points_within_CW = points_within_CW.sort_values(by='time')\n",
    "    \n",
    "    # Create a GeoDataFrame for the first intersections using the same CRS\n",
    "    geometry = gpd.points_from_xy(points_within_CW['longitude'], points_within_CW['latitude'], crs=CW_geo_crs)\n",
    "    first_intersections_CW = gpd.GeoDataFrame(points_within_CW, geometry=geometry, crs=CW_geo_crs)\n",
    "    \n",
    "    # Create a hexbin plot with the consistent grid and accumulate counts\n",
    "    hb = ax.hexbin(first_intersections_CW.geometry.x, first_intersections_CW.geometry.y, gridsize=100, cmap=pink_cmap, norm=LogNorm(), reduce_C_function=np.sum, zorder = 40)\n",
    "    bin_counts = hb.get_array()\n",
    "    for i, count in enumerate(bin_counts):\n",
    "        accumulated_bin_counts[i] += count\n",
    "    \n",
    "    # Calculate the total number of particles that intersect the shoreline buffer\n",
    "    total_particles_intersecting_CW += first_intersections_CW['group_id'].nunique()\n",
    "    \n",
    "    # Calculate the total number of particles tracked in the month\n",
    "    total_particles_tracked += combined_gdf['group_id'].nunique()\n",
    "    total_particles_list.append(combined_gdf.shape[0])\n",
    "    points_within_CW_list.append(first_intersections_CW.shape[0])\n",
    "\n",
    "# Calculate the average bin counts\n",
    "average_bin_counts = accumulated_bin_counts / len(monthly_data)\n",
    "\n",
    "# Calculate the average number of particles entering the shoreline buffer\n",
    "average_particles_entering_CW = total_particles_intersecting_CW / len(monthly_data)\n",
    "\n",
    "# Calculate the percentage of particles from the total trajectories that remain in the shoreline buffer\n",
    "percentage_particles_remain_in_CW = (total_particles_intersecting_CW / total_particles_tracked) * 100\n",
    "\n",
    "print(f'Average number of particles entering the Coastal Wetland: {average_particles_entering_CW}')\n",
    "print(f'Percentage of particles from total trajectories that remain in the Coastal Wetland: {percentage_particles_remain_in_CW:.2f}%')\n",
    "\n",
    "# Update the hexbin plot with average bin counts\n",
    "hb = ax.hexbin(*zip(*hexbin_grid), gridsize=100, C=average_bin_counts, cmap=pink_cmap, norm=LogNorm(), zorder=40)\n",
    "\n",
    "# Print the average number of particles in each bin\n",
    "print(\"Average number of particles in each bin:\")\n",
    "for i, count in enumerate(average_bin_counts):\n",
    "    print(f\"Bin {i}: {count:.2f} particles\")\n",
    "\n",
    "# Set the title of the colormap to the average number of particles that fall within the polygons\n",
    "#ax.set_title('Average number of particles returning to the coastal wetland over Winter 2023')\n",
    "\n",
    "# Add x and y labels\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(hb, ax=ax, pad=0.09)\n",
    "cbar.set_label('Average Number of Particles in each bin', fontsize=15)\n",
    "\n",
    "# Adjust layout to make space for colorbar\n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a list of purple shades\n",
    "#purple_shades = ['purple', 'violet', 'plum', 'thistle', 'mediumslateblue', 'darkorchid', 'mediumpurple', 'slateblue']\n",
    "blue_shades = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594']\n",
    "#blue_shades= ['royalblue','slateblue', 'mediumslateblue','mediumslateblue', 'mediumpurple','mediumorchid','plum']\n",
    "\n",
    "# Create a custom colormap\n",
    "blue_cmap = LinearSegmentedColormap.from_list('custom_blue', blue_shades)\n",
    "# plot hexbin map of coastal wetland with the color range of purple shades and the name of cmpap is Coastal Wetland density \n",
    "plt.figure(figsize=(20, 18))\n",
    "CW_geo.plot.hexbin(x='Start_Lon', y='Start_Lat', gridsize=30, cmap=blue_cmap,  norm=colors.LogNorm())\n",
    "plt.gca().set_facecolor('lightsteelblue')\n",
    "#plt.title('Coastal Wetlands Density')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.savefig('/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/results/CoastalWetlandsDensity.png')\n",
    "plt.show()\n",
    "# save the plot to a data_dir directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bathymetry data\n",
    "grid_metrics_file_name = f'{input_dir}/gridfile/grid_metrics_huron_senseflux_Seasonal_winter.nc'\n",
    "ds = Dataset(grid_metrics_file_name, 'r')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, LogNorm\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# Define a list of pink shades for the colormap\n",
    "pink_shades = ['#fff5f7', '#ffebf0', '#ffd6e1', '#ffbfd4', '#ff99c1', '#ff6ea9', '#ff4c92', '#ff2171', '#b50d4e']\n",
    "# Create a custom colormap\n",
    "pink_cmap = LinearSegmentedColormap.from_list('custom_pink', pink_shades)\n",
    "\n",
    "# Define a list of blue shades for the colormap and reverse it\n",
    "blue_shades = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594']\n",
    "blue_shades_reversed = blue_shades[::-1]\n",
    "\n",
    "# Define a list of green shades for the colormap\n",
    "green_shades = ['#e8f5e9', '#c8e6c9', '#a5d6a7', '#81c784', '#66bb6a', '#4caf50', '#43a047', '#388e3c', '#2e7d32']\n",
    "# Create a custom colormap\n",
    "green_cmap = LinearSegmentedColormap.from_list('custom_green', green_shades)\n",
    "\n",
    "# Create a custom colormap\n",
    "blue_cmap_reversed = LinearSegmentedColormap.from_list('custom_blue_reversed', blue_shades_reversed)\n",
    "font_size = 15\n",
    "cmap = colourmap('h_r')\n",
    "\n",
    "# Create the figure and axis with FVCOM plotter\n",
    "fig, ax = create_figure( figure_size=(26.,26.),projection=ccrs.PlateCarree(), font_size=font_size, bg_color='gray')\n",
    "\n",
    "# Load bathymetry data\n",
    "grid_metrics_file_name = '/home/abolmaal/data/FVCOME_OUTPUT/Simulations/Huron/input/gridfile/grid_metrics_huron_senseflux_Seasonal_winter.nc'\n",
    "ds = Dataset(grid_metrics_file_name, 'r')\n",
    "bathy = -ds.variables['h'][:]\n",
    "ds.close()\n",
    "# Configure plotter\n",
    "plotter = FVCOMPlotter(grid_metrics_file_name,\n",
    "                       geographic_coords=True,\n",
    "                       font_size=font_size)\n",
    "\n",
    "# Plot bathymetry\n",
    "#extents = np.array([-84.10,-84.20, 45.58,45.65], dtype=float)\n",
    "#extensts = np.array([-84,-81.3, 43,46], dtype=float)\n",
    "#Lake Huron Ausable Point\n",
    "extents = np.array([275, 277.69, 43, 46.3], dtype=float)\n",
    "ax, plot = plotter.plot_field(ax, bathy, extents=extents, add_colour_bar=True, cb_label='Depth (m)',\n",
    "                              vmin=-60., vmax=0., cmap=blue_cmap_reversed, zorder = 0)\n",
    "\n",
    "# Overlay grid\n",
    "plotter.draw_grid(ax, linewidth=1.0)\n",
    "\n",
    "\n",
    "# Plot the coastal wetland data with hexbin\n",
    "hb = ax.hexbin(CW_geo['Start_Lon'], CW_geo['Start_Lat'], gridsize=100, cmap=green_cmap, norm=LogNorm(), zorder=40)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(hb, ax=ax, pad=0.09)\n",
    "cbar.set_label('Coastal Wetland Density',fontsize=15)\n",
    "\n",
    "# Adjust layout to make space for colorbar\n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
